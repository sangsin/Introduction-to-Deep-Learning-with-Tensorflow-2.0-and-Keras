{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A friendly Introduction to CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRziSzL6z9q-",
        "outputId": "6f4ec26f-f503-4dbf-b869-71d62defdac7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np                   # advanced math library\n",
        "import matplotlib.pyplot as plt      # MATLAB like plotting routines\n",
        "import random                        # for generating random numbers\n",
        "\n",
        "from tensorflow.keras.models import Sequential  # Model type to be used\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\n",
        "from tensorflow.keras.datasets import mnist     # MNIST dataset is included in Keras\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "\n",
        "print(tf.__version__) # find the version number (should be 2.x+)\n",
        "\n",
        "# 그래피카드 유무 확인 및 메모리 확장 설정\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  print('사용가능한 GPU 갯수: ',len(gpus), '\\n')\n",
        "      \n",
        "  try:\n",
        "    # 프로그램이 실행되어 더 많은 GPU 메모리가 필요하면, 텐서플로 프로세스에 할당된 GPU 메모리 \n",
        "    # 영역을 확장할 수있도록 허용\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "  except RuntimeError as e:\n",
        "    # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
        "    print(e)\n",
        "\n",
        "# 설치된 GPU 상세내용 확인\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n",
            "사용가능한 GPU 갯수:  1 \n",
            "\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 10668336740783936191\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15395979264\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 17538955630016785288\n",
            "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Aunv2x0M5X",
        "outputId": "6acdb2b7-f9d9-4300-8192-1c722bfa946f"
      },
      "source": [
        "# Step 1: Data Preparation\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# Reshape\n",
        "image_size= x_train.shape[1]\n",
        "\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "\n",
        "# Normalizing\n",
        "\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "\n",
        "# One-hot encoding\n",
        "\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWc6HMQV0b2Z",
        "outputId": "273c451a-0cd5-4f1b-93a0-25a3404cc297"
      },
      "source": [
        "# Step 2: Model construction\n",
        "\n",
        "input_shape =(image_size,image_size, 1)\n",
        "batch_size = 128\n",
        "kernel_size=3\n",
        "pool_size=2\n",
        "filters=64\n",
        "dropout=0.2\n",
        "\n",
        "# model is a stack of CNN-ReLU-MaxPooling\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=filters, \n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu',\n",
        "                 input_shape= input_shape))\n",
        "model.add(MaxPooling2D(pool_size))\n",
        "\n",
        "model.add(Conv2D(filters=filters, \n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size))\n",
        "\n",
        "model.add(Conv2D(filters=filters, \n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu',\n",
        "                 input_shape= input_shape))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# dropout added as regularizer\n",
        "model.add(Dropout(dropout)) \n",
        "\n",
        "# output layer is 10-dim one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                5770      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 80,266\n",
            "Trainable params: 80,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go5NDPNw1Mde"
      },
      "source": [
        "# Step 3: Model compile\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydiWIHE21pqd",
        "outputId": "c6d5a40f-7aab-473a-c30f-04f8e4fddc16"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# Step 4: Model fit\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n",
        "\n",
        "end = time.time()\n",
        "print('Execution time in seconds =',end-start)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0148 - accuracy: 0.9953\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0143 - accuracy: 0.9954\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0118 - accuracy: 0.9959\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0115 - accuracy: 0.9961\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0106 - accuracy: 0.9965\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0085 - accuracy: 0.9972\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0090 - accuracy: 0.9968\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0075 - accuracy: 0.9974\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0077 - accuracy: 0.9976\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0066 - accuracy: 0.9976\n",
            "Execution time in seconds = 18.56520652770996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5V5NxVD1sVr",
        "outputId": "e5930318-6d53-4069-97d3-dce0b584d462"
      },
      "source": [
        "# The model evaluation output shows a maximum test accuracy of 99.3%, which can be achieved for a 3-layer network with \n",
        "# 64 feature maps per layer using the Adam optimizer with dropout=0.2 . CNNs are more parameter efficient and have a higher\n",
        "# accuracy than MLPs. \n",
        "# Likewise, CNNs are also suitable for learning representations from sequential data, images, and videos.\n",
        " \n",
        "_, acc = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                   verbose=0)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test accuracy: 99.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3XP60JF1wbQ"
      },
      "source": [
        "# How do Convolutional Neural Networks work?\n",
        "\n",
        "Link 1: [How do Convolutional Neural Networks work?](https://e2eml.school/how_convolutional_neural_networks_work.html)\n",
        "\n",
        "Link 2: [Youtube Video](https://www.youtube.com/watch?v=FmpDIaiMIeA&t=103s)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q23_yeZlXGU9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLOOP_ghI7Zd"
      },
      "source": [
        "# 작동원리\n",
        "\n",
        "  1. Convolution\n",
        "  2. Pooling\n",
        "  3. ReLU Activation\n",
        "  4. Deep Learning\n",
        "  5. Fully Connected Layers\n",
        "\n",
        "## Convolution\n",
        "\n",
        "아래 그림과 같이 컴퓨터에게 두 이미지중 하나를 제시하고 X와 O를 구분하게 하는 단순한 모형을 생각해보자.\n",
        "\n",
        "<img src = 'https://e2eml.school/images/cnn1.png'>\n",
        "\n",
        "제시되는 이미지는 축소(shrunken), 약간의 변형(deformed), 이동(shifted), 회전(rotation)이 된 그림일 수도 있다.\n",
        "\n",
        "<img src = \"https://e2eml.school/images/cnn2.png\">\n",
        "\n",
        "컴퓨터에게 이미지는 각 pixel에 숫자가 들어간 2차원 행렬로 인식된다. 우리 예에서는 흰색은 1, 흑색은 -1로 표시하였다.\n",
        "\n",
        "**Features (Patterns, Representations)**\n",
        "\n",
        "<img src='https://e2eml.school/images/cnn3.png'>\n",
        "\n",
        "CNN은 이미지를 여러개의 조각(patch, piece)으로 나누어 piece-by-piece로 패턴을 비교합니다.\n",
        "\n",
        "  - CNNs get a lot better at seeing similarity than whole-image matching schemes.\n",
        "  \n",
        "<img src = 'https://e2eml.school/images/cnn4.png'>\n",
        "\n",
        "  - Each feature is like a mini-image—a small two-dimensional array of values.\n",
        "  - Features match common aspects of the images.\n",
        "\n",
        "X의 경우 대각선(Diagonal Lines) feature와 크로스(X) feature가 그림에서 나온 여러개의 패치에서 일치 또는 유사한 경우를 찾게될 것이다.\n",
        "\n",
        "1. Convolution\n",
        "\n",
        "이미지가 처음 제시되었을 때 CNN은 정확히 어느 부분에 주어진 Feature가 match되는지 알 수 없기때문에 부분부분 옮겨 가면서 일치 또는 유사 여부를 확인하게 된다.이 과정에 사용되는 수학이 Convolution(합성곱)이며 이때 부분 부분 옮겨 가면서 사용되는 Feature를 필터(Filters)라고 한다.\n",
        "\n",
        "<img src=\"https://e2eml.school/images/cnn6.png\">\n",
        "\n",
        "\n",
        "**합성곱 연산**\n",
        "\n",
        "    ```\n",
        "    To calculate the match of a feature to a patch of the image, \n",
        "      (1) simply multiply each pixel in the feature by the value of the corresponding pixel in the image. \n",
        "      (2) Then add up the answers and divide by the total number of pixels in the feature. \n",
        "    ```\n",
        "\n",
        "- Every matching pixel results in 1. \n",
        "- Similarly, any mismatch is  -1. \n",
        "- If all the pixels in a feature match, then adding them up and dividing by the total number of pixels gives 1. \n",
        "- Similarly, if none of the pixels in a feature match the image patch, then the answer is -1.\n",
        "\n",
        "이렇게 해서 얻어진 2차원 행렬은 적용된 필터와 일치/유사/불일치하는지를 보여주는 지도(map)라 할 수 있으며 또한 원이미지에 필터를 적용한 축소된 이미지라 할 수있다.\n",
        "\n",
        "  - Values close to 1 show strong matches\n",
        "  - Values close to -1 show strong matches for the photographic negative of our feature \n",
        "  - Values near zero show no match of any sort.\n",
        "\n",
        "이 과정을 우리가 설정한 모든 필터(우리의 예에서는 3개)에 적용하면 이미지 셋( a set of 3 filtered images)을 구할 수 있다.\n",
        "\n",
        "<img src='https://e2eml.school/images/cnn7.png'>\n",
        "\n",
        "\n",
        "    ```\n",
        "    It’s easy to see how CNNs get their reputation as computation hogs.\n",
        "    Although we can sketch our CNN on the back of a napkin, the number of additions, multiplications and divisions can add up fast. \n",
        "    ```\n",
        "\n",
        "## Pooling\n",
        "\n",
        "  Pooling은 CNN의 핵심 요소중 하나로서 큰 사이즈의 이미지를 중요한 정보는 유지하면서 축소시키려는 기법이다.\n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn8.png'>\n",
        "\n",
        "  - In practice, a window 2 or 3 pixels on a side and steps of 2 pixels work well.\n",
        "  - After pooling, an image has about a quarter as many pixels as it started with. \n",
        "  - Because it keeps the maximum value from each window, it preserves the best fits of each feature within the window.\n",
        "  - A pooling layer is just the operation of performing pooling on an image or a collection of images. \n",
        "  - The output will have the same number of images, but they will each have fewer pixels. \n",
        "  - This is also helpful in managing the computational load. \n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn9.png'>\n",
        "\n",
        "\n",
        "## Rectified Linear Units (ReLU)\n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn10.png'>\n",
        "  \n",
        "  - A small but important player in this process is the Rectified Linear Unit or ReLU. \n",
        "  - It’s math is also very simple—wherever a negative number occurs, swap it out for a 0. \n",
        "    \n",
        "$$\n",
        "R(z) =  max(z,0) =\n",
        "  \\begin{cases}\n",
        "\t\t\t0, & \\text{for $z \\lt 0$}\\\\\n",
        "      z, & \\text{for $z \\ge 0$}\n",
        "\t\\end{cases}\n",
        "$$\n",
        "\n",
        "  - This helps the CNN stay mathematically healthy by keeping learned values from getting stuck near 0 or blowing up toward infinity. It’s the axle grease of CNNs.\n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn11.png'>\n",
        "\n",
        "  The output of a ReLU layer is the same size as whatever is put into it, just with all the negative values removed.\n",
        "\n",
        "\n",
        "## Deep learning\n",
        "\n",
        "  <img sr = 'https://e2eml.school/images/cnn12.png'>\n",
        "\n",
        "  - The input to each layer (two-dimensional arrays) looks a lot like the output (two-dimensional arrays). Because of this, we can stack them like Lego bricks. \n",
        "  - Raw images get filtered, rectified and pooled to create a set of shrunken, feature-filtered images. These can be filtered and shrunken again and again.\n",
        "  - Each time, the features become larger and more complex, and the images become more compact. \n",
        "  - This lets lower layers represent simple aspects of the image, such as edges and bright spots. \n",
        "  - Higher layers can represent increasingly sophisticated aspects of the image, such as shapes and patterns. These tend to be readily recognizable. For instance, in a CNN trained on human faces, the highest layers represent patterns that are clearly face-like. \n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn18.png'>\n",
        "\n",
        "  ## Fully connected layers (Dense Layers)\n",
        "\n",
        "  Fully connected layers take the high-level filtered images and translate them into votes. In our case, we only have to decide between two categories, X and O. \n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn13.png'>\n",
        "\n",
        "When a new image is presented to the CNN, it percolates through the lower layers until it reaches the fully connected layer at the end. Then an election is held. \n",
        "\n",
        "However, the process isn’t entirely democratic. Some values are much better than others at knowing when the image is an X, and some are particularly good at knowing when the image is an O. These get larger votes than the others. These votes are expressed as weights, or connection strengths, between each value and each category.\n",
        "\n",
        "In practice, several fully connected layers are often stacked together, with each intermediate layer voting on phantom “hidden” categories. In effect, each additional layer lets the network learn ever more sophisticated combinations of features that help it make better decisions.\n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn14.png'>\n",
        "\n",
        "## Backpropagation\n",
        "\n",
        "Our story is filling in nicely, but it still has a huge hole—Where do features come from? and How do we find the weights in our fully connected layers?\n",
        "\n",
        "If these all had to be chosen by hand, CNNs would be a good deal less popular than they are. Luckily, a bit of machine learning magic called backpropagation does this work for us.\n",
        "\n",
        "We start with an untrained CNN where every pixel of every feature and every weight in every fully connected layer is set to a random value. Then we feed images through it, one after other.\n",
        "\n",
        "Each image the CNN processes results in a vote. The amount of wrongness in the vote, the error, tells us how good our features and weights are. The features and weights can then be adjusted to make the error less. Each value is adjusted a little higher and a little lower, and the new error computed each time. Whichever adjustment makes the error less is kept. After doing this for every feature pixel in every convolutional layer and every weight in every fully connected layer, the new weights give an answer that works slightly better for that image. This is then repeated with each subsequent image in the set of labeled images.\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "Unfortunately, not every aspect of CNNs can be learned in so straightforward a manner. There is still a long list of decisions that a CNN designer must make.\n",
        "\n",
        "For each convolution layer, How many features? How many pixels in each feature?\n",
        "For each pooling layer, What window size? What stride?\n",
        "For each extra fully connected layer, How many hidden neurons?\n",
        "In addition to these there are also higher level architectural decisions to make: How many of each layer to include?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww3FAZ4hH6HD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}