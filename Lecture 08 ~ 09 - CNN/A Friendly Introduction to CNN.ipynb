{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Friendly Introduction to CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRziSzL6z9q-",
        "outputId": "324107a4-6e42-407a-f1d0-e6b30ae53612"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np                   # advanced math library\n",
        "import matplotlib.pyplot as plt      # MATLAB like plotting routines\n",
        "import random                        # for generating random numbers\n",
        "\n",
        "from tensorflow.keras.models import Sequential  # Model type to be used\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\n",
        "from tensorflow.keras.datasets import mnist     # MNIST dataset is included in Keras\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "\n",
        "print(tf.__version__) # find the version number (should be 2.x+)\n",
        "\n",
        "# 그래피카드 유무 확인 및 메모리 확장 설정\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  print('사용가능한 GPU 갯수: ',len(gpus), '\\n')\n",
        "      \n",
        "  try:\n",
        "    # 프로그램이 실행되어 더 많은 GPU 메모리가 필요하면, 텐서플로 프로세스에 할당된 GPU 메모리 \n",
        "    # 영역을 확장할 수있도록 허용\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "  except RuntimeError as e:\n",
        "    # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
        "    print(e)\n",
        "\n",
        "# 설치된 GPU 상세내용 확인\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "사용가능한 GPU 갯수:  1 \n",
            "\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8416743133503447878\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 16185556992\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 7819126513496997259\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Aunv2x0M5X"
      },
      "source": [
        "# Step 1: Data Preparation\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# Reshape\n",
        "image_size= x_train.shape[1]\n",
        "\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "\n",
        "# Normalizing\n",
        "\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "\n",
        "# One-hot encoding\n",
        "\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWc6HMQV0b2Z",
        "outputId": "39f79555-3a41-4869-d5ae-255d39955081"
      },
      "source": [
        "# Step 2: Model construction\n",
        "\n",
        "input_shape =(image_size,image_size, 1)\n",
        "batch_size = 128\n",
        "kernel_size=3\n",
        "pool_size=2\n",
        "filters=64\n",
        "dropout=0.2\n",
        "\n",
        "# model is a stack of CNN-ReLU-MaxPooling\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=filters, \n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu',\n",
        "                 input_shape= input_shape))\n",
        "model.add(MaxPooling2D(pool_size))\n",
        "\n",
        "model.add(Conv2D(filters=filters, \n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size))\n",
        "\n",
        "model.add(Conv2D(filters=filters, \n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# dropout added as regularizer\n",
        "model.add(Dropout(dropout)) \n",
        "\n",
        "# output layer is 10-dim one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                5770      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 80,266\n",
            "Trainable params: 80,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go5NDPNw1Mde"
      },
      "source": [
        "# Step 3: Model compile\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydiWIHE21pqd",
        "outputId": "ba2f5f4d-ab1e-4a72-8de1-bba5c4c539c1"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# Step 4: Model fit\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n",
        "\n",
        "end = time.time()\n",
        "print('Execution time in seconds =',end-start)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 3s 4ms/step - loss: 0.2847 - accuracy: 0.9124\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0730 - accuracy: 0.9776\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0531 - accuracy: 0.9839\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0423 - accuracy: 0.9863\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0349 - accuracy: 0.9890\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0294 - accuracy: 0.9904\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0267 - accuracy: 0.9916\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0222 - accuracy: 0.9930\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0213 - accuracy: 0.9932\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0185 - accuracy: 0.9941\n",
            "Execution time in seconds = 22.152608633041382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5V5NxVD1sVr",
        "outputId": "3dcf231c-ea67-4880-a12e-d069f8d9ce46"
      },
      "source": [
        "# The model evaluation output shows a maximum test accuracy of 99.3%, which can be achieved for a 3-layer network with \n",
        "# 64 feature maps per layer using the Adam optimizer with dropout=0.2 . CNNs are more parameter efficient and have a higher\n",
        "# accuracy than MLPs. \n",
        "# Likewise, CNNs are also suitable for learning representations from sequential data, images, and videos.\n",
        " \n",
        "_, acc = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                   verbose=0)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test accuracy: 99.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3XP60JF1wbQ"
      },
      "source": [
        "# How do Convolutional Neural Networks work?\n",
        "\n",
        "\n",
        "__VGG-16__\n",
        "> VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20200219152207/new41.jpg'>\n",
        "              \n",
        "---     \n",
        "         \n",
        "\n",
        "Link 1: [How do Convolutional Neural Networks work?](https://e2eml.school/how_convolutional_neural_networks_work.html)\n",
        "\n",
        "Link 2: [Youtube Video](https://www.youtube.com/watch?v=FmpDIaiMIeA&t=103s)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLOOP_ghI7Zd"
      },
      "source": [
        "## 작동원리\n",
        "\n",
        "  1. Convolution\n",
        "  2. Pooling\n",
        "  3. ReLU Activation\n",
        "  4. Deep Learning\n",
        "  5. Fully Connected Layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_TOa6hUGGTs"
      },
      "source": [
        "### Convolution\n",
        "\n",
        "아래 그림과 같이 컴퓨터에게 두 이미지중 하나를 제시하고 X와 O를 구분하게 하는 단순한 모형을 생각해보자.\n",
        "\n",
        "<img src = 'https://e2eml.school/images/cnn1.png'>\n",
        "\n",
        "제시되는 이미지는 축소(shrunken), 약간의 변형(deformed), 이동(shifted), 회전(rotation)이 된 그림일 수도 있다.\n",
        "\n",
        "<img src = \"https://e2eml.school/images/cnn2.png\">\n",
        "\n",
        "컴퓨터에게 이미지는 각 pixel에 숫자가 들어간 2차원 행렬로 인식된다. 우리 예에서는 흰색은 1, 흑색은 -1로 표시하였다.\n",
        "\n",
        "**Features (Patterns, Representations)**\n",
        "\n",
        "<img src='https://e2eml.school/images/cnn3.png'>\n",
        "\n",
        "CNN은 이미지를 여러개의 조각(patch, piece)으로 나누어 piece-by-piece로 패턴을 비교합니다.\n",
        "\n",
        "  - CNNs get a lot better at seeing similarity than whole-image matching schemes.\n",
        "  \n",
        "<img src = 'https://e2eml.school/images/cnn4.png'>\n",
        "\n",
        "  - Each feature is like a mini-image—a small two-dimensional array of values.\n",
        "  - Features match common aspects of the images.\n",
        "\n",
        "X의 경우 대각선(Diagonal Lines) feature와 크로스(X) feature가 그림에서 나온 여러개의 패치에서 일치 또는 유사한 경우를 찾게될 것이다.\n",
        "\n",
        "1. Convolution\n",
        "\n",
        "이미지가 처음 제시되었을 때 CNN은 정확히 어느 부분에 주어진 Feature가 match되는지 알 수 없기때문에 부분부분 옮겨 가면서 일치 또는 유사 여부를 확인하게 된다.이 과정에 사용되는 수학이 Convolution(합성곱)이며 이때 부분 부분 옮겨 가면서 사용되는 Feature를 필터(Filters)라고 한다.\n",
        "\n",
        "<img src=\"https://e2eml.school/images/cnn6.png\">\n",
        "\n",
        "\n",
        "**합성곱 연산**\n",
        "\n",
        "    ```\n",
        "    To calculate the match of a feature to a patch of the image, \n",
        "      (1) simply multiply each pixel in the feature by the value of the corresponding pixel in the image. \n",
        "      (2) Then add up the answers and divide by the total number of pixels in the feature. \n",
        "    ```\n",
        "\n",
        "- Every matching pixel results in 1. \n",
        "- Similarly, any mismatch is  -1. \n",
        "- If all the pixels in a feature match, then adding them up and dividing by the total number of pixels gives 1. \n",
        "- Similarly, if none of the pixels in a feature match the image patch, then the answer is -1.\n",
        "\n",
        "이렇게 해서 얻어진 2차원 행렬은 적용된 필터와 일치/유사/불일치하는지를 보여주는 지도(map)라 할 수 있으며 또한 원이미지에 필터를 적용한 축소된 이미지라 할 수있다.\n",
        "\n",
        "  - Values close to 1 show strong matches\n",
        "  - Values close to -1 show strong matches for the photographic negative of our feature \n",
        "  - Values near zero show no match of any sort.\n",
        "\n",
        "이 과정을 우리가 설정한 모든 필터(우리의 예에서는 3개)에 적용하면 이미지 셋( a set of 3 filtered images)을 구할 수 있다.\n",
        "\n",
        "<img src='https://e2eml.school/images/cnn7.png'>\n",
        "\n",
        "\n",
        "    ```\n",
        "    It’s easy to see how CNNs get their reputation as computation hogs.\n",
        "    Although we can sketch our CNN on the back of a napkin, the number of additions, multiplications and divisions can add up fast. \n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA1_fo7fGRi4"
      },
      "source": [
        "### Pooling\n",
        "\n",
        "  Pooling은 CNN의 핵심 요소중 하나로서 큰 사이즈의 이미지를 중요한 정보는 유지하면서 축소시키려는 기법이다.\n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn8.png'>\n",
        "\n",
        "  - In practice, a window 2 or 3 pixels on a side and steps of 2 pixels work well.\n",
        "  - After pooling, an image has about a quarter as many pixels as it started with. \n",
        "  - Because it keeps the maximum value from each window, it preserves the best fits of each feature within the window.\n",
        "  - A pooling layer is just the operation of performing pooling on an image or a collection of images. \n",
        "  - The output will have the same number of images, but they will each have fewer pixels. \n",
        "  - This is also helpful in managing the computational load. \n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn9.png'>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCZQoz3RGYaW"
      },
      "source": [
        "### Rectified Linear Units (ReLU)\n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn10.png'>\n",
        "  \n",
        "  - A small but important player in this process is the Rectified Linear Unit or ReLU. \n",
        "  - It’s math is also very simple—wherever a negative number occurs, swap it out for a 0. \n",
        "    \n",
        "$$\n",
        "R(z) =  max(z,0) =\n",
        "  \\begin{cases}\n",
        "\t\t\t0, & \\text{for $z \\lt 0$}\\\\\n",
        "      z, & \\text{for $z \\ge 0$}\n",
        "\t\\end{cases}\n",
        "$$\n",
        "\n",
        "  - This helps the CNN stay mathematically healthy by keeping learned values from getting stuck near 0 or blowing up toward infinity. It’s the axle grease of CNNs.\n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn11.png'>\n",
        "\n",
        "  The output of a ReLU layer is the same size as whatever is put into it, just with all the negative values removed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6HQFKQYGezl"
      },
      "source": [
        "### Deep learning\n",
        "\n",
        "  <img sr = 'https://e2eml.school/images/cnn12.png'>\n",
        "\n",
        "  - The input to each layer (two-dimensional arrays) looks a lot like the output (two-dimensional arrays). Because of this, we can stack them like Lego bricks. \n",
        "  - Raw images get filtered, rectified and pooled to create a set of shrunken, feature-filtered images. These can be filtered and shrunken again and again.\n",
        "  - Each time, the features become larger and more complex, and the images become more compact. \n",
        "  - This lets lower layers represent simple aspects of the image, such as edges and bright spots. \n",
        "  - Higher layers can represent increasingly sophisticated aspects of the image, such as shapes and patterns. These tend to be readily recognizable. For instance, in a CNN trained on human faces, the highest layers represent patterns that are clearly face-like. \n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn18.png'>\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCnw0KkVGk4b"
      },
      "source": [
        "### Fully connected layers (Dense Layers)\n",
        "\n",
        "  Fully connected layers take the high-level filtered images and translate them into votes. In our case, we only have to decide between two categories, X and O. \n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn13.png'>\n",
        "\n",
        "When a new image is presented to the CNN, it percolates through the lower layers until it reaches the fully connected layer at the end. Then an election is held. \n",
        "\n",
        "However, the process isn’t entirely democratic. Some values are much better than others at knowing when the image is an X, and some are particularly good at knowing when the image is an O. These get larger votes than the others. These votes are expressed as weights, or connection strengths, between each value and each category.\n",
        "\n",
        "In practice, several fully connected layers are often stacked together, with each intermediate layer voting on phantom “hidden” categories. In effect, each additional layer lets the network learn ever more sophisticated combinations of features that help it make better decisions.\n",
        "\n",
        "  <img src='https://e2eml.school/images/cnn14.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8qSWx0lGw36"
      },
      "source": [
        "### Backpropagation\n",
        "\n",
        "Our story is filling in nicely, but it still has a huge hole—Where do features come from? and How do we find the weights in our fully connected layers?\n",
        "\n",
        "If these all had to be chosen by hand, CNNs would be a good deal less popular than they are. Luckily, a bit of machine learning magic called backpropagation does this work for us.\n",
        "\n",
        "We start with an untrained CNN where every pixel of every feature and every weight in every fully connected layer is set to a random value. Then we feed images through it, one after other.\n",
        "\n",
        "Each image the CNN processes results in a vote. The amount of wrongness in the vote, the error, tells us how good our features and weights are. The features and weights can then be adjusted to make the error less. Each value is adjusted a little higher and a little lower, and the new error computed each time. Whichever adjustment makes the error less is kept. After doing this for every feature pixel in every convolutional layer and every weight in every fully connected layer, the new weights give an answer that works slightly better for that image. This is then repeated with each subsequent image in the set of labeled images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp8QJjbwG5tP"
      },
      "source": [
        "### Hyperparameters\n",
        "\n",
        "Unfortunately, not every aspect of CNNs can be learned in so straightforward a manner. There is still a long list of decisions that a CNN designer must make.\n",
        "\n",
        "For each convolution layer, How many features? How many pixels in each feature?\n",
        "For each pooling layer, What window size? What stride?\n",
        "For each extra fully connected layer, How many hidden neurons?\n",
        "In addition to these there are also higher level architectural decisions to make: How many of each layer to include?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xWpO-Y8vKdu",
        "outputId": "c9b37a37-3d06-4192-b842-e0ff373b773a"
      },
      "source": [
        "# Calculate O for the first covolutional layer(Conv2D) for our model\n",
        "\n",
        "''' \n",
        "  kernel_size(K) = 3\n",
        "  pool_size(P) = 2\n",
        "  filters(N) = 64 \n",
        "'''\n",
        "\n",
        "I=28\n",
        "K= 3\n",
        "N = 64\n",
        "S = 1\n",
        "P = 0\n",
        "\n",
        "O = (I-K+2*P)/S +1\n",
        "O"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCX7SFjcFmIX"
      },
      "source": [
        "## CNN의 parameter 개수와 tensor 사이즈 계산하기\n",
        "\n",
        "네트워크의 텐서 사이즈와 파라미터의 갯수를 계산하는 공식에 대해 다루려 한다.\n",
        "아래의 AlexNet을 이용하여 예시를 든다.\n",
        "\n",
        "<img src = 'https://seongkyun.github.io/assets/post_img/study/2019-01-25-num_of_parameters/fig1.png'>\n",
        "\n",
        "### AlexNet의 구조\n",
        "\n",
        "  - Input: 227* 227*3 크기의 컬러 이미지. \n",
        "  - Conv-1: 11*11 크기의 커널 96개, stride=4, padding=0\n",
        "  - MaxPool-1: stride 2, 3*3 max pooling layer\n",
        "  - Conv-2: 5*5 크기의 커널 256개, stride=1, padding=2\n",
        "  - MaxPool-2: stride 2, 3*3 max pooling layer\n",
        "  - Conv-3: 3*3 크기의 커널 384개, stride=1, padding=1\n",
        "  - Conv-4: 3*3 크기의 커널 384개, stride=1, padding=1\n",
        "  - Conv-5: 3*3 크기의 커널 256개, stride=1, -   padding=1\n",
        "  - Maxpool-3: stride 2, 3*3 max pooling layer\n",
        "  - FC-1: 4096개의 fully connected layer\n",
        "  - FC-2: 4096개의 fully connected layer\n",
        "  - FC-3: 1000개의 fully connected layer\n",
        "\n",
        "__특징__\n",
        "\n",
        "  - Relu activation function is used instead of Tanh to add non-linearity. It accelerates the speed by 6 times at the same accuracy.\n",
        "  - Use dropout instead of regularisation to deal with overfitting. However, the training time is doubled with the dropout rate of 0.5.\n",
        "  - Overlap pooling to reduce the size of the network. It reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively. \n",
        "\n",
        "\n",
        "__AlexNet의 총 parameter 개수 및 출력 tensor size__\n",
        "\n",
        "  - AlexNet의 전체 parameter 수는 5개의 convolution layer와 3개의 FC layer에서 계산되는 parameter 개수들의 합\n",
        "    - 62,378,344 개.\n",
        "  - 자세한 parameter 및 tensor size는 아래 표 참조\n",
        "\n",
        "\n",
        "  <img src ='https://seongkyun.github.io/assets/post_img/study/2019-01-25-num_of_parameters/fig2.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQapbo0THiaE"
      },
      "source": [
        "### Convolution layer의 output tensor size\n",
        "\n",
        "각각 기호를 아래와 같이 정의\n",
        "  - $O$: Size(width) of output image\n",
        "  - $I$: Size(width) of input image\n",
        "  - $K$: Size(width) of kernels used in the Conv layer\n",
        "  - $N$: Number of kernels\n",
        "  - $S$: Stride of the convolution operation\n",
        "  - $P$: Padding size\n",
        "\n",
        "__$O$(width of output image)는 다음과 같이 계산__\n",
        "\n",
        "$$ O = \\frac{I-K+2P}{S}+1$$ \n",
        "\n",
        "  - 출력 이미지의 채널 수는 커널의 갯수($N$)와 같음\n",
        "\n",
        "__MaxPool layer의 output tensor size__\n",
        "\n",
        "각각 기호를 아래와 같이 정의\n",
        "  - $O$: Size(width) of output image\n",
        "  - $I$: Size(width) of input image\n",
        "  - $S$: Stride of the convolution operation\n",
        "  - $P_s$: Pooling size\n",
        "\n",
        "$$ O = \\frac{I-P_s}{S}+1$$ \n",
        "\n",
        "Convolution layer와는 다르게 출력의 채널 수는 입력의 개수와 동일  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aIp0Pp3w1vB"
      },
      "source": [
        "___      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLKLfSi73REF"
      },
      "source": [
        "First Convolution Layer의 출력 이미지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSV7qJtU1YqF",
        "outputId": "69d5e945-2c4f-4d41-9e0e-9753b3600d79"
      },
      "source": [
        "# Calculate O for the first convolutional layer for Alexnet architectue\n",
        "I = 227\n",
        "N = 96\n",
        "K = 11\n",
        "S = 4\n",
        "P = 0\n",
        "\n",
        "O = int((I-K+2*P)/S +1)\n",
        "print('First Convolution Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('First Convolution Layer의 출력 이미지의 shape =',(O,O,N))\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Convolution Layer의 출력 이미지의 크기(width) = 55\n",
            "First Convolution Layer의 출력 이미지의 shape = (55, 55, 96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UEpV8xGrKK5"
      },
      "source": [
        "First Pooling Layer의 출력 이미지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ub5VIJp225",
        "outputId": "d3d9fdc7-4dd8-4086-e04e-bdec5fc6481d"
      },
      "source": [
        "# Calculate O for the first pooling layer for Alexnet architectue\n",
        "I = O\n",
        "S = 2\n",
        "P = 0\n",
        "P_s = 3\n",
        "\n",
        "O = int((I-P_s)/S +1)\n",
        "print('First Pooling Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('First Pooling Layer의 출력 이미지의 shape =',(O,O,N))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Pooling Layer의 출력 이미지의 크기(width) = 27\n",
            "First Pooling Layer의 출력 이미지의 shape = (27, 27, 96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnG5GQLOq7BQ"
      },
      "source": [
        "Second Convolution Layer의 출력 이미지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2J-er_1SefP",
        "outputId": "87cec1ad-e8d5-492f-e2db-9dc796952a02"
      },
      "source": [
        "# Calculate O for the second covolutional layer for Alexnet architectue\n",
        "\n",
        "I = O\n",
        "K = 5\n",
        "N = 256\n",
        "S = 1\n",
        "P = 2\n",
        "\n",
        "O = int((I-K+2*P)/S +1)\n",
        "print('Second Conv Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('Second Conv Layer의 출력 이미지의 shape =',(O,O,N))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Second Conv Layer의 출력 이미지의 크기(width) = 27\n",
            "Second Conv Layer의 출력 이미지의 shape = (27, 27, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwL-26rKr51i"
      },
      "source": [
        "Second pooling Layer의 출력 이미지는 (27,27,256)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSALejNstLDp",
        "outputId": "965666cd-f841-4cf8-fb1c-892b0522e2c0"
      },
      "source": [
        "# Calculate O for the second pooling layer for Alexnet architectue\n",
        "I = O\n",
        "S = 2\n",
        "P = 0\n",
        "P_s = 3\n",
        "\n",
        "O = int((I-P_s)/S +1)\n",
        "print('Second Pooling Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('Second Pooling Layer의 출력 이미지의 shape =',(O,O,N))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Second Pooling Layer의 출력 이미지의 크기(width) = 13\n",
            "Second Pooling Layer의 출력 이미지의 shape = (13, 13, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuHC62iPt249"
      },
      "source": [
        "Third Conv Layer의 출력 이미지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vEmXrFj3BTT",
        "outputId": "82240440-a875-4421-9e00-eb2b70788c7c"
      },
      "source": [
        "# Calculate O for the third conv layer for Alexnet architectue\n",
        "I = O\n",
        "K = 3\n",
        "N = 384\n",
        "S = 1\n",
        "P = 1\n",
        "\n",
        "O = int((I-K+2*P)/S +1)\n",
        "print('Third Conv Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('Third Conv Layer의 출력 이미지의 shape =',(O,O,N))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Third Conv Layer의 출력 이미지의 크기(width) = 13\n",
            "Third Conv Layer의 출력 이미지의 shape = (13, 13, 384)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TDqocpr9y5G"
      },
      "source": [
        "The 4th Conv Layer의 출력 이미지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex2Wt4ig974u",
        "outputId": "c1c95afa-eebb-4361-f018-4af5916f32a6"
      },
      "source": [
        "# Calculate O for the 4th conv layer for Alexnet architectue\n",
        "I = O\n",
        "K = 3\n",
        "N = 384\n",
        "S = 1\n",
        "P = 1\n",
        "\n",
        "O = int((I-K+2*P)/S +1)\n",
        "print('4th Conv Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('4th Conv Layer의 출력 이미지의 shape =',(O,O,N))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4th Conv Layer의 출력 이미지의 크기(width) = 13\n",
            "4th Conv Layer의 출력 이미지의 shape = (13, 13, 384)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKewFzds-bDP"
      },
      "source": [
        "The 5th Conv Layer의 출력 이미지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vQsRhgp-kbu",
        "outputId": "e8b06215-c558-4e56-e138-971f1274b9e0"
      },
      "source": [
        "# Calculate O for the 5th conv layer for Alexnet architectue\n",
        "I = O\n",
        "K = 3\n",
        "N = 256\n",
        "S = 1\n",
        "P = 1\n",
        "\n",
        "O = int((I-K+2*P)/S +1)\n",
        "print('5th Conv Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('5th Conv Layer의 출력 이미지의 shape =',(O,O,N))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5th Conv Layer의 출력 이미지의 크기(width) = 13\n",
            "5th Conv Layer의 출력 이미지의 shape = (13, 13, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgQVXtq4-vGI"
      },
      "source": [
        "# The last Pooling Layer의 출력 이미지"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l0k8llI-2CK",
        "outputId": "917635ab-48eb-49d8-b86f-e6db8b4d2f4a"
      },
      "source": [
        "# Calculate O for the last pooling layer for Alexnet architectue\n",
        "I = O\n",
        "S = 2\n",
        "P = 0\n",
        "P_s = 3\n",
        "\n",
        "O = int((I-P_s)/S +1)\n",
        "print('Last Pooling Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('Last Pooling Layer의 출력 이미지의 shape =',(O,O,N))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last Pooling Layer의 출력 이미지의 크기(width) = 6\n",
            "Last Pooling Layer의 출력 이미지의 shape = (6, 6, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIHzDCFMAj0E"
      },
      "source": [
        "### Convolution layer의 parameter 갯수\n",
        "\n",
        "  - CNN의 각 layer는 weight parameter와 bias parameter가 존재.\n",
        "  - 전체 네트워크의 parameter 수는 각 conv layer 파라미터 수의 합\n",
        "\n",
        "각각 기호를 아래와 같이 정의\n",
        "\n",
        "  - $W_c$: Number of weights of the Conv layer\n",
        "  - $B_c$: Number of biases of the Conv layer\n",
        "  - $P_c$: Number of parameters of the Conv layer\n",
        "  - $K$: Size(width) of kernels used in the Conv layer\n",
        "  - $N$: Number of kernels\n",
        "  - $C$: Number of channels of the input image\n",
        "\n",
        "  $$W_c = K^2 \\times C \\times N$$\n",
        "  $$B_c = N$$\n",
        "  $$P_c = W_c + B_c$$\n",
        "\n",
        "  - Conv layer에서 모든 커널의 깊이는 항상 입력 이미지의 채널 수와 같음\n",
        "  - 따라서 모든 커널에는 $K^2\\times C$개의 parameter들이 있으며, 그러한 커널들이 $N$개 존재"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzNgDDyFCbZE"
      },
      "source": [
        "AlexNet의 Conv-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtKI2tArCkTB",
        "outputId": "6649ef15-3670-4855-ee45-d905495b2943"
      },
      "source": [
        "C = 3\n",
        "K = 11\n",
        "N = 96\n",
        "\n",
        "Wc = K**2*C*N\n",
        "Bc = N\n",
        "Pc = Wc+Bc\n",
        "\n",
        "Wc, Bc, Pc"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34848, 96, 34944)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LqF7PwMD2hF"
      },
      "source": [
        "__Fully Connnected layer의 parameter 갯수__\n",
        "\n",
        "#### Case1: FC layer connected to a Conv layer\n",
        "\n",
        "각각의 기호를 아래와 같이 정의\n",
        "  - $W_{cf}$: Number of weights of a FC layer which is connected to a Conv layer\n",
        "  - $B_{cf}: Number of biases of a FC layer which is connected to a Conv layer\n",
        "  - $P_{cf}: Number of parameters of a FC layer which is connected to a Conv layer\n",
        "  - $O$: Size(width) of th output image of the previous Conv layer\n",
        "  - $N$: Number of kernels in the previous Conv layer\n",
        "  - $F$: Number of neurons in the FC Layer\n",
        "\n",
        "  $$W_{cf} = O^2 \\times N \\times F$$\n",
        "  $$B_{cf} = F$$\n",
        "  $$P_{cf} = W_{cf} + B_{cf}$$\n",
        "\n",
        "#### Case2: FC layer connected to a FC Layer\n",
        "각각의 기호를 아래와 같이 정의\n",
        "  - $W_{ff}$: Number of weights of a FC layer which is connected to a FC layer\n",
        "  - $B_{ff}$: Number of biases of a FC layer which is connected to a FC layer\n",
        "  - $P_{ff}$: Number of parameters of a FC layer which is connected to a FC layer\n",
        "  - $F$: Number of neurons in th FC layer\n",
        "  - $F_{-1}$: Number of neurons in the previous FC layer\n",
        "\n",
        "  $$W_{ff} = F_{-1} \\times F$$\n",
        "  $$B_{ff} = F$$\n",
        "  $$P_{ff} = W_{ff} + B_{ff}$$\n",
        "\n",
        "\n",
        "위의 식에서, $W_{ff} = F_{-1} \\times F$는 이전 FC layer의 neuron과 현재 FC layer의 neuron 사이의 총 연결 가중치의 개수.\n",
        "Bias parameter의 개수는 뉴런의 개수()와 같음\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_5aoaVgGY5G"
      },
      "source": [
        "Conv layer의 마지막단에 바로 붙는 FC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbcLZiRjGgFB",
        "outputId": "7a3d5ce8-2ff8-4756-c3ed-8da4cc01e63c"
      },
      "source": [
        "O = 6\n",
        "N = 256\n",
        "F = 4096\n",
        "\n",
        "W_cf = O**2*N*F\n",
        "B_cf = F\n",
        "P_cf = W_cf+B_cf\n",
        "\n",
        "W_cf, B_cf, P_cf\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(37748736, 4096, 37752832)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stXk4JAEHMI2"
      },
      "source": [
        "이 수는 모든 Conv layer의 pameter 갯수들보다 많은 수(그만큼 FC layer에는 많은 파라미터들이 필요)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N7owSG3HVpB"
      },
      "source": [
        "마지막 FC layer인 FC-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ROXiTPYHZ1l",
        "outputId": "8dcf92b4-f372-4794-e80b-d901834a390f"
      },
      "source": [
        "F_1 = 4096\n",
        "F = 1000\n",
        "W_ff = F_1*F\n",
        "B_ff = F\n",
        "P_ff =W_ff+B_ff\n",
        "\n",
        "W_ff, B_ff, P_ff"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4096000, 1000, 4097000)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNdqQJxXH8dC"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu1zKw-6afO9"
      },
      "source": [
        "#### 앞에서 살펴본 mnist CNN 모델에 대한 출력 shaped와 파라미터에 대해 알아보자. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRRjLibAS6dG",
        "outputId": "6442b7dd-e18c-4b96-a6d6-f016bce4269a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                5770      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 80,266\n",
            "Trainable params: 80,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVxXV4vZUSM5",
        "outputId": "1ca1475c-19fe-47ca-c89b-02c248eb7999"
      },
      "source": [
        "# Calculate O for the first covolutional layer for Our model\n",
        "''' \n",
        "  kernel_size(K) = 3\n",
        "  pool_size(P) = 2\n",
        "  filters(N) = 64 \n",
        "'''\n",
        "\n",
        "I=28\n",
        "K= 3\n",
        "N = 64\n",
        "S = 1\n",
        "# P = 0\n",
        "\n",
        "O = int((I-K+2*P)/S +1)\n",
        "print('first Conv Layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('first Conv Layer의 출력 이미지의 shape =',(O,O,N))\n",
        "\n",
        "C = 1\n",
        "\n",
        "Wc = K**2*C*N\n",
        "Bc = N\n",
        "Pc = Wc+Bc\n",
        "\n",
        "print('first Conv Layer의 파라미터 갯수 =',Pc)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first Conv Layer의 출력 이미지의 크기(width) = 26\n",
            "first Conv Layer의 출력 이미지의 shape = (26, 26, 64)\n",
            "first Conv Layer의 파라미터 갯수 = 640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjObxFlmwCuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013e2611-3960-4dbc-a82e-4d17d23768d6"
      },
      "source": [
        "# Calculate O for the first maxpooling layer for our model\n",
        "\n",
        "I = O\n",
        "K = 3\n",
        "N = 64\n",
        "\n",
        "# P = 0\n",
        "P_s = 2\n",
        "S = 2 # In Keras, stride가 선언되어 있지 않으면 디폴트 값은 Pool Size.\n",
        "      # max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n",
        "\n",
        "O = int((I-P_s)/S +1)\n",
        "print('the first maxpooling layer의 출력 이미지의 크기(width) =',int(O))\n",
        "print('the first maxpooling layer의 출력 이미지의 shape =',(O,O,N))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the first maxpooling layer의 출력 이미지의 크기(width) = 13\n",
            "the first maxpooling layer의 출력 이미지의 shape = (13, 13, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXAJuGvLg_F0"
      },
      "source": [
        "__model.summary()와 같은 결과가 나오는지 확인해 보세요.__"
      ]
    }
  ]
}