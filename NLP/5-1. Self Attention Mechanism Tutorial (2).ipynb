{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-1. Self Attention Mechanism Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XQ6NsIuDtgr"
      },
      "source": [
        "# Illustrated: Self-Attention\n",
        "Step-by-step guide to self-attention with illustrations and code\n",
        "\n",
        "[medium article](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
        "\n",
        "\n",
        "> Colab made by: [Manuel Romero](https://twitter.com/mrm8488)\n",
        "\n",
        "Ï∞∏Í≥†\n",
        "\n",
        "0. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "1. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "2.  [Transformer Î™®Îç∏ Î∂ÑÏÑù (Self-Attention)](https://wdprogrammer.tistory.com/72)\n",
        "3. [Ïñ¥ÌÖêÏÖò Î©îÏª§ÎãàÏ¶òÍ≥º transfomer(self-attention)](https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225)\n",
        "4. [Why multi-head self attention works: math, intuitions and 10+1 hidden insights](https://theaisummer.com/self-attention/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX0Iv3KFdFtQ"
      },
      "source": [
        "\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*_92bnsMJy8Bl539G4v93yg.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOkXKd60Q_Iu"
      },
      "source": [
        "What do *BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, MobileBERT, TinyBERT and CamemBERT* all have in common? And I‚Äôm not looking for the answer ‚ÄúBERT‚Äù ü§≠.\n",
        "Answer: **self-attention** ü§ó. We are not only talking about architectures bearing the name ‚ÄúBERT‚Äô, but more correctly **Transformer-based architectures**. Transformer-based architectures, which are primarily used in modelling language understanding tasks, eschew the use of recurrence in neural network (RNNs) and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs. But what‚Äôs the math behind this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yozzTBjBRbAA"
      },
      "source": [
        "The main content of this kernel is to walk you through the mathematical operations involved in a self-attention module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atUYzU3TSD9z"
      },
      "source": [
        "### Step 0. What is self-attention?\n",
        "\n",
        "If you‚Äôre thinking if self-attention is similar to attention, then the answer is yes! They fundamentally share the same concept and many common mathematical operations.\n",
        "A self-attention module takes in n inputs, and returns n outputs. What happens in this module? In layman‚Äôs terms, the self-attention mechanism allows the inputs to interact with each other (‚Äúself‚Äù) and find out who they should pay more attention to (‚Äúattention‚Äù). The outputs are aggregates of these interactions and attention scores.\n",
        "\n",
        "<img src= 'https://pbs.twimg.com/media/Et1JSjMVEAAGgLT?format=png&name=900x900'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Es4rpLohY-"
      },
      "source": [
        "### Self-Attention at a High Level\n",
        "\n",
        "Say the following sentence is an input sentence we want to translate:\n",
        "\n",
        "‚ÄùThe animal didn't cross the street because it was too tired‚Äù\n",
        "\n",
        "What does ‚Äúit‚Äù in this sentence refer to? Is it referring to the street or to the animal? It‚Äôs a simple question to a human, but not as simple to an algorithm.\n",
        "\n",
        "When the model is processing the word ‚Äúit‚Äù, self-attention allows it to associate ‚Äúit‚Äù with ‚Äúanimal‚Äù.\n",
        "\n",
        "As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
        "\n",
        "If you‚Äôre familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it‚Äôs processing. Self-attention is the method the Transformer uses to bake the ‚Äúunderstanding‚Äù of other relevant words into the one we‚Äôre currently processing.\n",
        "\n",
        "<img src='http://jalammar.github.io/images/t/transformer_self-attention_visualization.png'>\n",
        "\n",
        "As we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsBtMAiLhh2R"
      },
      "source": [
        "### Self-Attention in Detail\n",
        "\n",
        "\n",
        "**The first step** in calculating self-attention is to create three vectors from each of the encoder‚Äôs input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
        "\n",
        "<img src='http://jalammar.github.io/images/t/transformer_self_attention_vectors.png'>\n",
        "\n",
        "  - Multiplying x1 by the $W^Q$ weight matrix produces $q1$, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence.\n",
        "\n",
        "**The second step** in calculating self-attention is to calculate a score. \n",
        "\n",
        "Say we‚Äôre calculating the self-attention for the first word in this example, ‚ÄúThinking‚Äù.\n",
        "\n",
        "<img src='http://jalammar.github.io/images/t/transformer_self_attention_score.png'>\n",
        "\n",
        "  -  We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
        "\n",
        "__The third and forth steps__ are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper ‚Äì 64. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they‚Äôre all positive and add up to 1.\n",
        "\n",
        "<img src= 'http://jalammar.github.io/images/t/self-attention_softmax.png'>\n",
        "  \n",
        "  - This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it‚Äôs useful to attend to another word that is relevant to the current word.\n",
        "\n",
        "__The fifth step__ is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n",
        "\n",
        "__The sixth step__ is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n",
        "\n",
        "<img src='http://jalammar.github.io/images/t/self-attention-output.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH_1Rdj8lM3a"
      },
      "source": [
        "### The Math: __\"Self-attention as two matrix multiplications\"__\n",
        "\n",
        "Given our well know inputs:\n",
        "\n",
        "$$\\textbf{X} \\in R^{batch \\times tokens \\times dim}$$\n",
        " \n",
        "And trainable weight matrices:\n",
        "\n",
        "$$\\textbf{W}_{i}^{Q}, \\textbf{W}_{i}^{K}, \\textbf{W}_{i}^{V} \\in {R}^{d_{\\text{model}} \\times d_{k}}$$\n",
        "\n",
        "__First Step__\n",
        "\n",
        "We create 3 distinct representations ( the query, the key, and the value):\n",
        "\n",
        "$$\\textbf{Q} = \\textbf{X} \\textbf{W}^Q, \\textbf{K} = \\textbf{X} \\textbf{W}^K, \\textbf{V} = \\textbf{X} \\textbf{W}^V$$ \n",
        "\n",
        "<img src='http://jalammar.github.io/images/t/self-attention-matrix-calculation.png'>\n",
        "\n",
        "\n",
        "__Second Step__\n",
        "\n",
        "Then, we can define content-based (self) attention as:\n",
        "\n",
        "$$\\operatorname{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V})=\\operatorname{softmax}\\left(\\frac{\\textbf{Q} \\textbf{K}^{T}}{\\sqrt{d_{k}}}\\right) \\textbf{V}$$\n",
        "\n",
        "Where the attention scoring function is calculated using the dot-product similarity and is happening right here:\n",
        "\n",
        "$$\\operatorname{Dot-scores} = \\left(\\frac{\\textbf{Q} \\textbf{K}^{T}}{\\sqrt{d_{k}}}\\right)$$\n",
        "\n",
        "<img src='http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png'>\n",
        "\n",
        "#### The Query-Key matrix multiplication\n",
        "\n",
        "Content-based attention has distinct representations. \n",
        "\n",
        "  - The query matrix in the attention layer is conceptually the ‚Äúsearch‚Äù in the database. \n",
        "  - The keys will account for where we will be looking while the values will actually give us the desired content. \n",
        "  - Consider the keys and values as components of our database.\n",
        "\n",
        "Intuitively, __the keys are the bridge between the queries (what we are looking for) and the values (what we will actually get).__\n",
        "\n",
        "Keep in mind that each vector to vector multiplication is a dot-product \n",
        "similarity. __We can use the keys to guide our ‚Äúsearch‚Äù and tell us where to look with respect to the input elements.__\n",
        "\n",
        "In other words, the keys will account for the computation of the attention on how to weigh the values based on our particular queries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDMmHAaSTE6P"
      },
      "source": [
        "Following, we are going to explain and implement:\n",
        "- Prepare inputs\n",
        "- Initialise weights\n",
        "- Derive key, query and value\n",
        "- Calculate attention scores for Input 1\n",
        "- Calculate softmax\n",
        "- Multiply scores with values\n",
        "- Sum weighted values to get Output 1\n",
        "- Repeat steps 4‚Äì7 for Input 2 & Input 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1UxPJlHBVmS"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENdzUZqSBsiB"
      },
      "source": [
        "### Step 1: Prepare inputs\n",
        "\n",
        "For this tutorial, for the shake of simplicity, we start with 3 inputs, each with dimension 4.\n",
        "\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*hmvdDXrxhJsGhOQClQdkBA.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKYrJsljBhnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c38120-6ac9-4bc8-aa86-08bb46a76541"
      },
      "source": [
        "x = [\n",
        "  [1, 0, 1, 0], # Input 1\n",
        "  [0, 2, 0, 2], # Input 2\n",
        "  [1, 1, 1, 1]  # Input 3\n",
        " ]\n",
        "\n",
        "x = tf.constant(x, dtype=tf.float32)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 1. 0.]\n",
            " [0. 2. 0. 2.]\n",
            " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ96EoE1Bvat"
      },
      "source": [
        "### Step 2: Initialise weights\n",
        "\n",
        "Every input must have three representations (see diagram below). These representations are called **key** (orange), **query** (red), and **value** (purple). For this example, let‚Äôs take that we want these representations to have a dimension of 3. Because every input has a dimension of 4, this means each set of the weights must have a shape of 4√ó3.\n",
        "\n",
        "![texto del enlace](https://miro.medium.com/max/1975/1*VPvXYMGjv0kRuoYqgFvCag.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUTNr15JBkSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc27a2f9-712b-45a8-d78c-88968de21832"
      },
      "source": [
        "w_key = [\n",
        "  [0, 0, 1],\n",
        "  [1, 1, 0],\n",
        "  [0, 1, 0],\n",
        "  [1, 1, 0]\n",
        "]\n",
        "w_query = [\n",
        "  [1, 0, 1],\n",
        "  [1, 0, 0],\n",
        "  [0, 0, 1],\n",
        "  [0, 1, 1]\n",
        "]\n",
        "w_value = [\n",
        "  [0, 2, 0],\n",
        "  [0, 3, 0],\n",
        "  [1, 0, 3],\n",
        "  [1, 1, 0]\n",
        "]\n",
        "w_key = tf.Variable(w_key, dtype=tf.float32)\n",
        "w_query = tf.Variable(w_query, dtype=tf.float32)\n",
        "w_value = tf.Variable(w_value, dtype=tf.float32)\n",
        "\n",
        "print(\"Weights for key: \\n\", w_key)\n",
        "print(\"Weights for query: \\n\", w_query)\n",
        "print(\"Weights for value: \\n\", w_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights for key: \n",
            " <tf.Variable 'Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
            "array([[0., 0., 1.],\n",
            "       [1., 1., 0.],\n",
            "       [0., 1., 0.],\n",
            "       [1., 1., 0.]], dtype=float32)>\n",
            "Weights for query: \n",
            " <tf.Variable 'Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
            "array([[1., 0., 1.],\n",
            "       [1., 0., 0.],\n",
            "       [0., 0., 1.],\n",
            "       [0., 1., 1.]], dtype=float32)>\n",
            "Weights for value: \n",
            " <tf.Variable 'Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
            "array([[0., 2., 0.],\n",
            "       [0., 3., 0.],\n",
            "       [1., 0., 3.],\n",
            "       [1., 1., 0.]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pr9XZF9X_Ed"
      },
      "source": [
        "Note: *In a neural network setting, these weights are usually small numbers, initialised randomly using an appropriate random distribution like Gaussian, Xavier and Kaiming distributions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxGT5awVB1Xw"
      },
      "source": [
        "### Step 3: Derive key, query and value\n",
        "\n",
        "Now that we have the three sets of weights, let‚Äôs actually obtain the **key**, **query** and **value** representations for every input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQwhDIi7aGXp"
      },
      "source": [
        "Obtaining the keys:\n",
        "```\n",
        "               [0, 0, 1]\n",
        "[1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]\n",
        "[0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]\n",
        "[1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]\n",
        "```\n",
        "![texto alternativo](https://miro.medium.com/max/1975/1*dr6NIaTfTxEWzxB2rc0JWg.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi0EblXTamFz"
      },
      "source": [
        "Obtaining the values:\n",
        "```\n",
        "               [0, 2, 0]\n",
        "[1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] \n",
        "[0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]\n",
        "[1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]\n",
        "```\n",
        "![texto alternativo](https://miro.medium.com/max/1975/1*5kqW7yEwvcC0tjDOW3Ia-A.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTp2izu1bLNq"
      },
      "source": [
        "Obtaining the querys:\n",
        "```\n",
        "               [1, 0, 1]\n",
        "[1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]\n",
        "[0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]\n",
        "[1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]\n",
        "```\n",
        "![texto alternativo](https://miro.medium.com/max/1975/1*wO_UqfkWkv3WmGQVHvrMJw.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qegb9M0KbnRK"
      },
      "source": [
        "Notes: *Notes\n",
        "In practice, a bias vector may be added to the product of matrix multiplication.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv2NXynOB7oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49e685f-467d-4baf-b7f7-6f008cf683d0"
      },
      "source": [
        "keys = tf.matmul(x, w_key)\n",
        "querys = tf.matmul(x, w_query)\n",
        "values = tf.matmul(x, w_value)\n",
        "print(keys)\n",
        "print(querys)\n",
        "print(values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 1.]\n",
            " [4. 4. 0.]\n",
            " [2. 3. 1.]], shape=(3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 0. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 1. 3.]], shape=(3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [2. 8. 0.]\n",
            " [2. 6. 3.]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pmf0OQhCnD8"
      },
      "source": [
        "### Step 4: Calculate attention scores\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*u27nhUppoWYIGkRDmYFN2A.gif)\n",
        "\n",
        "To obtain **attention scores**, we start off with taking a dot product between Input 1‚Äôs **query** (red) with **all keys** (orange), including itself. Since there are 3 key representations (because we have 3 inputs), we obtain 3 attention scores (blue).\n",
        "\n",
        "```\n",
        "            [0, 4, 2]\n",
        "[1, 0, 2] x [1, 4, 3] = [2, 4, 4]\n",
        "            [1, 0, 1]\n",
        "```\n",
        "Notice that we only use the query from Input 1. Later we‚Äôll work on repeating this same step for the other querys.\n",
        "\n",
        "Note: *The above operation is known as dot product attention, one of the several score functions. Other score functions include scaled dot product and additive/concat.*            "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GDhKEl0Cokw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d9539a-5e20-48b5-bb43-5a36bcc41909"
      },
      "source": [
        "attn_scores = tf.matmul(querys, keys, transpose_b=True)\n",
        "print(attn_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 2.  4.  4.]\n",
            " [ 4. 16. 12.]\n",
            " [ 4. 12. 10.]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO3NmnbvCxpX"
      },
      "source": [
        "### Step 5: Calculate softmax\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*jf__2D8RNCzefwS0TP1Kyg.gif)\n",
        "\n",
        "Take the **softmax** across these **attention scores** (blue).\n",
        "```\n",
        "softmax([2, 4, 4]) = [0.0, 0.5, 0.5]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDNzdZHVC1ys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03892e96-01fe-4825-c3b1-0d4f9abab3d5"
      },
      "source": [
        "attn_scores_softmax = tf.nn.softmax(attn_scores, axis=-1)\n",
        "print(np.around(attn_scores_softmax,2))\n",
        "\n",
        "# For readability, approximate the above as follows\n",
        "attn_scores_softmax = [\n",
        "  [0.0, 0.5, 0.5],\n",
        "  [0.0, 1.0, 0.0],\n",
        "  [0.0, 0.9, 0.1]\n",
        "]\n",
        "attn_scores_softmax = tf.Variable(attn_scores_softmax)\n",
        "print(attn_scores_softmax)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.06 0.47 0.47]\n",
            " [0.   0.98 0.02]\n",
            " [0.   0.88 0.12]]\n",
            "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
            "array([[0. , 0.5, 0.5],\n",
            "       [0. , 1. , 0. ],\n",
            "       [0. , 0.9, 0.1]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBe71nseDBhb"
      },
      "source": [
        "### Step 6: Multiply scores with values\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*9cTaJGgXPbiJ4AOCc6QHyA.gif)\n",
        "\n",
        "The softmaxed attention scores for each input (blue) is multiplied with its corresponding **value** (purple). This results in 3 alignment vectors (yellow). In this tutorial, we‚Äôll refer to them as **weighted values**.\n",
        "```\n",
        "1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]\n",
        "2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]\n",
        "3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNnx-Fx5DFDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0aff25-0369-4595-955c-bbc789d8f5d3"
      },
      "source": [
        "weighted_values = values[:,None] * tf.transpose(attn_scores_softmax)[:,:,None]\n",
        "print(weighted_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0.  0.  0. ]\n",
            "  [0.  0.  0. ]\n",
            "  [0.  0.  0. ]]\n",
            "\n",
            " [[1.  4.  0. ]\n",
            "  [2.  8.  0. ]\n",
            "  [1.8 7.2 0. ]]\n",
            "\n",
            " [[1.  3.  1.5]\n",
            "  [0.  0.  0. ]\n",
            "  [0.2 0.6 0.3]]], shape=(3, 3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU6w0U9ADQIc"
      },
      "source": [
        "### Step 7: Sum weighted values\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*1je5TwhVAwwnIeDFvww3ew.gif)\n",
        "\n",
        "Take all the **weighted values** (yellow) and sum them element-wise:\n",
        "\n",
        "```\n",
        "  [0.0, 0.0, 0.0]\n",
        "+ [1.0, 4.0, 0.0]\n",
        "+ [1.0, 3.0, 1.5]\n",
        "-----------------\n",
        "= [2.0, 7.0, 1.5]\n",
        "```\n",
        "\n",
        "The resulting vector ```[2.0, 7.0, 1.5]``` (dark green) **is Output 1**, which is based on the **query representation from Input 1** interacting with all other keys, including itself.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3yNYDUEgAos"
      },
      "source": [
        "### Step 8: Repeat for Input 2 & Input 3\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*G8thyDVqeD8WHim_QzjvFg.gif)\n",
        "\n",
        "Note: *The dimension of **query** and **key** must always be the same because of the dot product score function. However, the dimension of **value** may be different from **query** and **key**. The resulting output will consequently follow the dimension of **value**.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6excNSUDRRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2077747-13fb-4bef-dfdf-8400c8e35c57"
      },
      "source": [
        "outputs = tf.reduce_sum(weighted_values, axis=0)  # 6\n",
        "print(outputs)\n",
        "\n",
        "# tensor([[2.0000, 7.0000, 1.5000],  # Output 1\n",
        "#         [2.0000, 8.0000, 0.0000],  # Output 2\n",
        "#         [2.0000, 7.8000, 0.3000]]) # Output 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[2.        7.        1.5      ]\n",
            " [2.        8.        0.       ]\n",
            " [2.        7.7999997 0.3      ]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}