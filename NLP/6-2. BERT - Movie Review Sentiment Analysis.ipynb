{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-3. TensorFlow 2 -BERT: Movie Review Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fQ69Sok9LJn"
      },
      "source": [
        "# What is BERT?\n",
        "\n",
        "  - BERT stands for Bidirectional Encoder Representations from Transformers. \n",
        "  - A pre-trained BERT model can be fine-tuned to create state-of-the-art (SOTA) models for a wide range of NLP tasks such as:\n",
        "    - Question Answering\n",
        "    - Sentiment Analysis \n",
        "    - Named Entity Recognition (NER). \n",
        "  - BERT Pretrained Models \n",
        "    - BASE has 110M parameters (L=12, H=768, A=12) \n",
        "    - BERT LARGE has 340M parameters (L=24, H=1024, A=16)(L stands for the number of layers, H for the hidden size and A for the number of self-attention heads) ([Devlin et al., 2019](https://arxiv.org/abs/1810.04805)).\n",
        "  - Model Architecture\n",
        "    - BERT model architecture is a multi-layer bidirectional Transformer encoder (see Figure 1). \n",
        "      <img src = 'https://miro.medium.com/max/736/1*IN-Z-o-9m9jAB57_xZX47A.png'>\n",
        "      \n",
        "      - The authors of BERT paper pre-train the model with 3.3 billion words in the two NLP tasks: \n",
        "        - Task #1: Masked LM (MLM) and \n",
        "        - Task #2: Next Sentence Prediction (NSP).\n",
        "      \n",
        "      - BERT model has an interesting input (see Figure 2) representation. \n",
        "        - Its input is the sum of the token embeddings, the segment embeddings and the position embeddings \n",
        "\n",
        "          <img src='https://miro.medium.com/max/3600/1*YgAWrY8PnFkncyDZW7ycPg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bh2eYFHLK1u"
      },
      "source": [
        "### [Loading models from TensorFlow Hub](https://www.tensorflow.org/text/tutorials/classify_text_with_bert#loading_models_from_tensorflow_hub)\n",
        "\n",
        "Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n",
        "\n",
        "  - BERT-Base, Uncased and seven more models with trained weights released by the original BERT authors.\n",
        "  - Small BERTs have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
        "  - ALBERT: four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n",
        "  - BERT Experts: eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n",
        "  - Electra has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n",
        "  - BERT with Talking-Heads Attention and Gated GELU [base, large] has two improvements to the core of the Transformer architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbt7XUq4L5FO"
      },
      "source": [
        "# Coding Practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNyYQLBZBp0s",
        "outputId": "0d4fbde5-e6e8-4a47-afb4-61b7500295cb"
      },
      "source": [
        "# Install the required package\n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prFU97m-B7w4",
        "outputId": "5c91c52e-3518-4798-f8f4-5934f1048875"
      },
      "source": [
        "# Import modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import bert\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)\n",
        "pd.set_option('display.max_colwidth',1000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.6.0\n",
            "Hub version:  0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbrmogNdMFwp"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjbjQBGsCCxe"
      },
      "source": [
        "# Read the IMDB Dataset.csv into Pandas dataframe\n",
        "url = 'https://raw.githubusercontent.com/bestvater/misc/master/IMDB%20Dataset.csv'\n",
        "df=pd.read_csv(url)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ae73GwCC36O",
        "outputId": "cf7ed176-50f1-491f-e6a1-0fc9a256c9cb"
      },
      "source": [
        "# Take a peek at the dataset\n",
        "df.head(5)\n",
        "print(df.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prr945l5DAzY",
        "outputId": "077ec3dc-dc7e-40d9-8937-1602811c4456"
      },
      "source": [
        "print(\"The number of rows and columns in the dataset is: {}\".format(df.shape))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of rows and columns in the dataset is: (50000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-yMDIp8DGIy",
        "outputId": "be36042e-0229-4eeb-f30c-a0caf25c0a61"
      },
      "source": [
        "# Identify missing values\n",
        "df.apply(lambda x: sum(x.isnull()), axis=0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "review       0\n",
              "sentiment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U-0oUuZDLJw",
        "outputId": "3812b497-2dbc-457d-a6b2-338040516f46"
      },
      "source": [
        "# Check the target class balance\n",
        "df[\"sentiment\"].value_counts()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    25000\n",
              "positive    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di7rSEwlMOY9"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqybZPY0DPiq"
      },
      "source": [
        "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
        "\n",
        "MAX_SEQ_LEN=500 # max sequence length\n",
        "\n",
        "def get_masks(tokens):\n",
        "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
        "    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        " \n",
        "def get_segments(tokens):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        "\n",
        "def get_ids(tokens, tokenizer):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
        "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def create_single_input(sentence, tokenizer, max_len):\n",
        "    \"\"\"Create an input from a sentence\"\"\"\n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "    stokens = stokens[:max_len]\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        " \n",
        "    ids = get_ids(stokens, tokenizer)\n",
        "    masks = get_masks(stokens)\n",
        "    segments = get_segments(stokens)\n",
        "\n",
        "    return ids, masks, segments\n",
        " \n",
        "def convert_sentences_to_features(sentences, tokenizer):\n",
        "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        " \n",
        "    for sentence in tqdm(sentences,position=0, leave=True):\n",
        "      ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n",
        "      assert len(ids) == MAX_SEQ_LEN\n",
        "      assert len(masks) == MAX_SEQ_LEN\n",
        "      assert len(segments) == MAX_SEQ_LEN\n",
        "      input_ids.append(ids)\n",
        "      input_masks.append(masks)\n",
        "      input_segments.append(segments)\n",
        "\n",
        "    return [np.asarray(input_ids, dtype=np.int32), \n",
        "          np.asarray(input_masks, dtype=np.int32), \n",
        "          np.asarray(input_segments, dtype=np.int32)]\n",
        "\n",
        "def create_tonkenizer(bert_layer):\n",
        "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
        "    vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n",
        "    tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
        "    return tokenizer"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKONcsFsDvsg",
        "outputId": "bae69dc8-a6e2-4360-a53e-8cf8febdbc3e"
      },
      "source": [
        "def nlp_model(callable_object):\n",
        "    # Load the pre-trained BERT base model\n",
        "    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \n",
        "   \n",
        "    # BERT layer three inputs: ids, masks and segments\n",
        "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \n",
        "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \n",
        "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "    inputs = [input_ids, input_masks, input_segments] # BERT inputs\n",
        "    pooled_output, sequence_output = bert_layer(inputs) # BERT outputs\n",
        "    \n",
        "    # Add a hidden layer\n",
        "    x = Dense(units=768, activation='relu')(pooled_output)\n",
        "    x = Dropout(0.1)(x)\n",
        " \n",
        "    # Add output layer\n",
        "    outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "    # Construct a new model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = nlp_model(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\")\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_4 (KerasLayer)      [(None, 768), (None, 109482241   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 768)          590592      keras_layer_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 768)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            1538        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 110,074,371\n",
            "Trainable params: 110,074,370\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxo5eCfHJlx9"
      },
      "source": [
        "**BERT Pretrained Model has been continuously updated.**\n",
        "\n",
        "Link: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVVk5XaXDyko"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4n5pcUMECBQ",
        "outputId": "e79dbe99-ed69-4465-c290-5919d86c8af5"
      },
      "source": [
        "# Create examples for training and testing\n",
        "df = df.sample(frac=1) # Shuffle the dataset\n",
        "tokenizer = create_tonkenizer(model.layers[3])\n",
        "X_train = convert_sentences_to_features(df['review'][:40000], tokenizer)\n",
        "X_test = convert_sentences_to_features(df['review'][40000:], tokenizer)\n",
        "\n",
        "df['sentiment'].replace('positive',1.,inplace=True)\n",
        "df['sentiment'].replace('negative',0.,inplace=True)\n",
        "one_hot_encoded = to_categorical(df['sentiment'].values)\n",
        "y_train = one_hot_encoded[:40000]\n",
        "y_test =  one_hot_encoded[40000:]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40000/40000 [03:19<00:00, 200.06it/s]\n",
            "100%|██████████| 10000/10000 [00:50<00:00, 199.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOuoDKH2JZHm",
        "outputId": "907cccdd-b0ab-4662-a8c2-9ff1969c86c8"
      },
      "source": [
        "# Train the model\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "\n",
        "# Use Adam optimizer to minimize the categorical_crossentropy loss\n",
        "opt = Adam(learning_rate=2e-5)\n",
        "model.compile(optimizer=opt, \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the data to the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    verbose = 1)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('nlp_model.h5')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000/5000 [==============================] - 8876s 2s/step - loss: 0.2119 - accuracy: 0.9151 - val_loss: 0.1806 - val_accuracy: 0.9381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-x1Tcy5EEfd"
      },
      "source": [
        "### Analysis of model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJwoGRylEYMN"
      },
      "source": [
        "# Load the pretrained nlp_model\n",
        "from tensorflow.keras.models import load_model\n",
        "new_model = load_model('nlp_model.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9qAg7EyEazi"
      },
      "source": [
        "# Predict on test dataset\n",
        "from sklearn.metrics import classification_report\n",
        "pred_test = np.argmax(new_model.predict(X_test), axis=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssYOTLrREeas",
        "outputId": "721c75e6-143c-46d1-9208-f348e33770c0"
      },
      "source": [
        "print(classification_report(np.argmax(y_test,axis=1), pred_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94      5058\n",
            "           1       0.94      0.93      0.94      4942\n",
            "\n",
            "    accuracy                           0.94     10000\n",
            "   macro avg       0.94      0.94      0.94     10000\n",
            "weighted avg       0.94      0.94      0.94     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBDMdA3hEhzc",
        "outputId": "b73f9a9c-c63b-4274-f8bb-1a7f8949f3c7"
      },
      "source": [
        "pred_test[:10]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0, 1, 0, 0, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GztMQDtEmcg",
        "outputId": "2553ff48-7e87-4668-8c46-ec25308ba7e2"
      },
      "source": [
        "# Predicted 1 for the first review in the test dataset\n",
        "df['review'][40000:40001]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38136    The greatest compliments to the other commentator here at IMDb who asked himself why this series didn't \"get stuck\" in its time to last a lot longer like many other series in the 80s did.<br /><br />It is not true the series would have gotten worse if further continued.<br /><br />I will at the end of this my comment post some thoughts about the other movie realizations, rather: attempts of the Robin Hood legend.<br /><br />First of All, Robert Addie (Gisburne), you are among us all, you live forever.<br /><br />Nothing is as fun as the entire two, if one wants, three seasons of this absolutely unique series. And at the same time absolutely agreeing with the mostly new and revolutionary findings of Terry Jones' history documentations about Egypt, Greece, Rome, Konstantinopel, the Goths and Barbarians, and the middle ages and crusades (...yes, THE Monthy Python-Terry Jones):<br /><br />If you have seen those brilliant and funny Jones-Docs you will better, much better understand all ...\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knvyY3KeEmtF",
        "outputId": "1feaa7df-cbde-4678-ab1c-da8e429c4d76"
      },
      "source": [
        "# Predicted 1 for the second review in the test dataset\n",
        "df['review'][40001:40002]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3203    Some people say the pace of this film is a little slow, but how is this different from any other Hitchcock movie? They all move very deliberately and, as a point, have spurts of suspense and brilliant montages injected through it. This movie gives us just the right amount of comic relief which make the suspense scenes seem all the more suspenseful. The Albert Hall scene is one of the best examples of Pure Cinema that exists in Hitchcock's collection (the best probably being almost all of \"Rear Window\"). Pure Cinema for Hitchcock meant a series of usually small pieces of film fit together without dialogue, in order to tell the story visually. This is, of course the basic definition of the Albert Hall sequence, as well as the shorter staircase sequence at the end of the picture. <br /><br />Not many slip-ups by Hitchcock here, and the acting is superb especially by Doris Day in a rather surprising serious role.\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2UABAR8Emwo",
        "outputId": "a43ef207-d6d6-427c-cafc-250479fc70bd"
      },
      "source": [
        "# Predicted 0 for the third review in the test dataset\n",
        "df['review'][40002:40003]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14277    I'm not surprised that this film did well at the Hamptons Film Festival. It is a shallow film that would appeal well to shallow people. Two actors pretending to be actors in a relationship who fight and look for a lost dog. The film is allegedly exploring the dynamics of the relationship, however, the relationship is far too petty to merit any such exploration. This couple has one dimension: they fight, they tease, then they make love and fight some more. There brief moment of tenderness does not reveal any possible reason that these two would be involved with each other given their venomous and volatile relationship. Beautifully shot, excellent score, but without anything of merit in the script or characters, this short is just that.\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU4HBy8Z1Jqf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}