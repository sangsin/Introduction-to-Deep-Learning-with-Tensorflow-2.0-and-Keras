{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-1. Recurrent Neural Networks (RNN) with Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-SAgCrXjfjP"
      },
      "source": [
        "# Introduction to RNN(Recurrent Neural Networks)\n",
        "\n",
        "- 기본 아이디어\n",
        "\n",
        "    > 인간은 모든 생각을 밑바닥부터 시작하지 않는다. \n",
        "    >\n",
        "    > 지금 이 글을 읽는 당신도 매 단어를 그 전 단어들을 바탕으로 이해할 것이 분명하다. \n",
        "    > 지금까지 봐왔던 것들을 모두 집어던지고 아무 것도 모르는 채로 생각하지 않을 것이다. 생각은 계속 나아가는 것이다.\n",
        "    > \n",
        "    > 전통적인 neural network이 이렇게 지속되는 생각을 하지 못한다는 것이 큰 단점이다.\n",
        "    >\n",
        "    > 예를 들어, 영화의 매 순간 일어나는 사건을 분류하고 싶다고 해보자. 전통적인 neural network는 이전에 일어난 사건을 바탕으로 나중에 일어나는 사건을 생각하지 못한다.\n",
        "    >\n",
        "    > Recurrent neural network (이하 RNN)는 이 문제를 해결하고자 하는 모델이다. RNN은 스스로를 반복하면서 이전 단계에서 얻은 정보가 지속되도록 한다.\n",
        "\n",
        "\n",
        "- RNN(Recurrent Neural Network)은 시퀀스(Sequence) 모델입니다. 입력과 출력을 시퀀스 단위로 처리하는 모델입니다. \n",
        "\n",
        "    - 번역기를 생각해보면 입력은 번역하고자 하는 문장. 즉, 단어 시퀀스입니다. 출력에 해당되는 번역된 문장 또한 단어 시퀀스입니다. 이러한 시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델이라고 합니다. 그 중에서도 RNN은 딥 러닝에 있어 가장 기본적인 시퀀스 모델입니다.\n",
        "\n",
        "    - 지난 몇 년 동안, RNN은 음성 인식, 언어 모델링, 번역, 이미지 주석 생성 등등등등의 다양한 분야에서 굉장한 성공을 거두었다. \n",
        "      - 참고: RNN으로 얻을 수 있는 놀라운 이점에 대한 논의는 Andrej Karpathy의 (The Unreasonable Effectiveness of Recurrent Neural Networks)[http://karpathy.github.io/2015/05/21/rnn-effectiveness/] \n",
        "\n",
        "- RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. \n",
        "- Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. \n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkCjjBR1Xahp"
      },
      "source": [
        "\n",
        "## 순환 신경망의 특징\n",
        "\n",
        "  - 입력 X를 받아서, 출력 Y를 반환합니다.\n",
        "  - 순환구조를 가지고 있다; 어떤 레이어의 출력을 다시 입력으로 받는 구조를 말합니다.\n",
        "  - 순환 신경망은 입력과 출력의 길이에 제한이 없습니다.\n",
        "  - 순환 신경망은 이미지에 대한 설명을 생성하는 이미지 설명 생성, 문장의 긍정/부정을 판단하는 감성 분석, 하나의 언어를 다른 언어로 번역하는 기계 번역(Machine Translation) 등 다양한 용도로 활용됩니다.\n",
        "\n",
        "<img src='https://i.stack.imgur.com/WSOie.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHOLu3yT0e0y"
      },
      "source": [
        "\n",
        "## Simple RNN \n",
        "\n",
        "### Model Architecture Diagram\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src='https://theaisummer.com/static/2c376937397d142197de07495803cb54/29007/rnn-cell-time-unfold.png'>\n",
        "\n",
        "\n",
        "__RNN Model Computation Mechanism__\n",
        "\n",
        "<img src = 'http://i.imgur.com/s8nYcww.png'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Simple RNN의 단점\n",
        "\n",
        "  - Vanishing Gradient Problem\n",
        "    - RNN은 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우 역전파시 그래디언트가 점차 줄어 학습능력이 크게 저하되는 문제\n",
        "\n",
        "<img src='http://i.imgur.com/H9UoXdC.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4F46uiV1OPK"
      },
      "source": [
        "## LSTM(Long Short Term Memory Networks)\n",
        "\n",
        "### Simple RNN vs LSTM\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile5.uf.tistory.com%2Fimage%2F99893B375ACB86A035DE41'>\n",
        "\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile30.uf.tistory.com%2Fimage%2F999F603E5ACB86A00550F0'>\n",
        "\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile5.uf.tistory.com%2Fimage%2F993A93495ACB86A02FFAA8'>\n",
        "\n",
        "### LSTM의 핵심 아이디어\n",
        "\n",
        "LSTM의 핵심은 cell state인데, 모듈 그림에서 수평으로 그어진 윗 선에 해당한다.\n",
        "\n",
        "Cell state는 컨베이어 벨트와 같아서, 작은 linear interaction만을 적용시키면서 전체 체인을 계속 구동시킨다. 정보가 전혀 바뀌지 않고 그대로 흐르게만 하는 것은 매우 쉽게 할 수 있다.\n",
        "\n",
        "LSTM은 cell state에 뭔가를 더하거나 없앨 수 있는 능력이 있는데, 이 능력은 gate라고 불리는 구조에 의해서 조심스럽게 제어된다.\n",
        "\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile29.uf.tistory.com%2Fimage%2F99C98C4F5ACB86A01FD4E5'>\n",
        "\n",
        "\n",
        "\n",
        "### 단계별로 보는 LSTM\n",
        "\n",
        "LSTM은 3개의 gate를 가지고 있고, 이 문들은 cell state를 보호하고 제어한다.   \n",
        "\n",
        "\n",
        "1. 첫 단계(forget gate layer)\n",
        "\n",
        "LSTM의 첫 단계로는 cell state로부터 어떤 정보를 버릴 것인지를 정하는 것으로, sigmoid layer에 의해 결정된다. 그래서 이 단계의 gate를 \"forget gate layer\"라고 부른다. 이 단계에서는 $h_{t-1}$과 $x_{t}$를 받아서 0과 1 사이의 값을 $C_{t-1}$에 보내준다. \n",
        "     \n",
        "\n",
        "  - 그 값이 1이면 \"모든 정보를 보존해라\"가 되고, 0이면 \"죄다 갖다버려라\"가 된다.\n",
        "      - 예를 들어 cell state는 현재 주어의 성별 정보를 가지고 있을 수도 있어서 그 성별에 맞는 대명사가 사용되도록 준비하고 있을 수도 있을 것이다. 그런데 새로운 주어가 왔을 때, 우리는 기존 주어의 성별 정보를 생각하고 싶지 않을 것이다.\n",
        "\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile2.uf.tistory.com%2Fimage%2F9957DB445ACB86A02155EA'>\n",
        "\n",
        "<h4><center>LSTM의 forget gate layer</center></h4>\n",
        "\n",
        "2. 두번째 단계(input gate layer)\n",
        "  - 다음 단계는 앞으로 들어오는 새로운 정보 중 어떤 것을 cell state에 저장할 것인지를 정한다. \n",
        "      - 먼저, \"input gate layer\"라고 불리는 sigmoid layer가 어떤 값을 업데이트할 지 정한다. \n",
        "      - 그 다음에 tanh layer가 새로운 후보 값들인 $\\widetilde{C}_t$ 라는 vector를 만들고, cell state에 더할 준비를 한다.\n",
        "      - 이렇게 두 단계에서 나온 정보를 합쳐서 state를 업데이트할 재료를 만들게 된다.\n",
        "      - 마지막으로 과거 state인 $C_{t-1}$를 업데이트해서 새로운 cell state인 $C_t$를 만든다. \n",
        "        * 예를 들어 다시 언어 모델의 예제에서, 기존 주어의 성별을 잊어버리기로 했고, 그 대신 새로운 주어의 성별 정보를 cell state에 더하고 싶을 것이다\n",
        "\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F99D969495ACB86A00BFC15'>\n",
        "\n",
        "<h4><center>LSTM의 input gate layer</center></h4>\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile9.uf.tistory.com%2Fimage%2F997589405ACB86A00CADEA'>\n",
        "\n",
        "<h4><center>LSTM의 cell state 업데이트</center></h4>\n",
        "\n",
        "3. 세번째 단계(output gate layer)\n",
        "  - 마지막으로 무엇을 output으로 내보낼 지 정하는 일이 남았다. \n",
        "      - 가장 먼저, sigmoid layer에 input 데이터를 태워서 cell state의 어느 부분을 output으로 내보낼 지를 정한다. \n",
        "      - 그리고나서 cell state를 tanh layer에 태워서 -1과 1 사이의 값을 받은 뒤에 방금 전에 계산한 sigmoid gate의 output과 곱해준다. \n",
        "\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F99FB824C5ACB86A10D4182'>\n",
        "\n",
        "<h4><center>LSTM의 output gate layer</center></h4>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ca65cda94c8"
      },
      "source": [
        "# Recurrent Neural Networks (RNN) with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjRFWwes64Z5",
        "outputId": "485de605-9483-4f86-b539-111136214bc0"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "print('시작시간:', time.ctime(start))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "시작시간: Thu Sep 16 01:22:55 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3600ee25c8e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71c626bbac35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c6c6b7-c0e3-49c9-ffac-2d1715f444cb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, SimpleRNN, LSTM, GRU, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# To prevent CUBLAS_STATUS_ALLOC_FAILED problem in tensorflow 2, the follwing codes are necessary.\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXvJ9MZqe_M0"
      },
      "source": [
        "# Step 1: Data Preparation\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "image_size= x_train.shape[1]\n",
        "\n",
        "# Reshaping for RNN \n",
        "\n",
        "x_train= np.reshape(x_train,[-1,image_size,image_size])\n",
        "x_test= np.reshape(x_test,[-1,image_size,image_size])\n",
        "\n",
        "# Normalizing\n",
        "x_train=x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "#One-hot encoding\n",
        "\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRYnNweNwdxg"
      },
      "source": [
        "## Simple RNN with mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzzOOms3gMwn"
      },
      "source": [
        "# Step 2: Model Building\n",
        "\n",
        "# Hyperparameter setup\n",
        "\n",
        "n_steps = 28     # 28 rows\n",
        "n_inputs = 28    # 28 cols\n",
        "input_shape = (n_steps, n_inputs)  #    =  (image_size,image_size)\n",
        "batch_size = 128\n",
        "n_units = 30    # Number of neurons in a cell\n",
        "dropout=0.2\n",
        "epochs= 20\n",
        "\n",
        "# model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=n_units,\n",
        "                    dropout=dropout,\n",
        "                    input_shape=input_shape))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbFz8D9BfDw5"
      },
      "source": [
        "There are two main differences between the RNN classifier and the two previous models, ANN & CNN.\n",
        "\n",
        "  - the input_shape = (image_size, image_size) , which is actually input_ shape = (timesteps, input_dim) or a sequence of input_dim -dimension vectors of timesteps length.\n",
        "  - Second is the use of a SimpleRNN layer to represent an RNN cell with units=256 . __The units variable represents the number of output units.__\n",
        "\n",
        "If the CNN is characterized by the convolution of kernels across the input feature map, __the RNN output is a function not only of the present input but also of the previous output or hidden state. Since the previous output is also a function of the previous input, the current output is also a function of the previous output and input and so on.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPLLKLZ5kkqa"
      },
      "source": [
        "# Step 3: Model Compile\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZbirdbGnKpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99e1466-adad-4a60-851f-35a275adadf8"
      },
      "source": [
        "# Step 4: Model Fit\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size = batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 12s 24ms/step - loss: 1.3098 - accuracy: 0.5537\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.8635 - accuracy: 0.7064\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.7381 - accuracy: 0.7466\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.6756 - accuracy: 0.7713\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 11s 22ms/step - loss: 0.6331 - accuracy: 0.7887\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.5969 - accuracy: 0.8071\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.5695 - accuracy: 0.8210\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 0.5347 - accuracy: 0.8359\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.4995 - accuracy: 0.8491\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.4712 - accuracy: 0.8586\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.4402 - accuracy: 0.8705\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.4145 - accuracy: 0.8783\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.3937 - accuracy: 0.8860\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.3714 - accuracy: 0.8924\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.3608 - accuracy: 0.8953\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.3440 - accuracy: 0.9007\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.3325 - accuracy: 0.9040\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.3278 - accuracy: 0.9055\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.3117 - accuracy: 0.9097\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.3067 - accuracy: 0.9111\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7ee0376990>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfAhl2odkrat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfeeb8e3-45e9-4867-b37a-11ed9efd3890"
      },
      "source": [
        "# Step 5: Model evaluation\n",
        "\n",
        "__, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"Test Accuracy: %.1f%%\" %(100*acc))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 4ms/step - loss: 0.2648 - accuracy: 0.9245\n",
            "Test Accuracy: 92.4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LLdOrkKGZiF"
      },
      "source": [
        "### How to improve Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZiawOguGkZB",
        "outputId": "008be7c7-bb3a-45fa-fe9f-c45c48de1235"
      },
      "source": [
        "# Hyperparameter setup\n",
        "\n",
        "n_steps = 28     # 28 rows\n",
        "n_inputs = 28    # 28 cols\n",
        "input_shape = (n_steps, n_inputs)  #    =  (image_size,image_size)\n",
        "batch_size = 128\n",
        "n_units = 256    # Number of neurons in a cell\n",
        "dropout=0.2\n",
        "epochs= 20\n",
        "\n",
        "# Step 2: Model Building\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=n_units,\n",
        "                    dropout=dropout,\n",
        "                    input_shape=input_shape))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Step 3: Model Compile\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Model Fit\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size = batch_size)\n",
        "\n",
        "# Step 5: Model evaluation\n",
        "\n",
        "__, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest Accuracy: %.1f%%\" %(100*acc))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 12s 23ms/step - loss: 0.4747 - accuracy: 0.8522\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.2272 - accuracy: 0.9307\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1812 - accuracy: 0.9459\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1609 - accuracy: 0.9522\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.1453 - accuracy: 0.9574\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.1358 - accuracy: 0.9595\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1286 - accuracy: 0.9613\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1247 - accuracy: 0.9634\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.1166 - accuracy: 0.9651\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1065 - accuracy: 0.9685\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1070 - accuracy: 0.9686\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1010 - accuracy: 0.9703\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1008 - accuracy: 0.9703\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0971 - accuracy: 0.9716\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0958 - accuracy: 0.9715\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.0910 - accuracy: 0.9728\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0909 - accuracy: 0.9730\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0918 - accuracy: 0.9732\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0893 - accuracy: 0.9739\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0865 - accuracy: 0.9744\n",
            "79/79 [==============================] - 0s 4ms/step - loss: 0.0850 - accuracy: 0.9768\n",
            "\n",
            "Test Accuracy: 97.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGvvhk72peYq"
      },
      "source": [
        "In many deep neural networks, other members of the RNN family are more commonly used. For example, Long Short-Term Memory ( LSTM ) has been used in both machine translation and question answering problems. LSTM addresses the problem of long-term dependency or remembering relevant past information to the present output.\n",
        "\n",
        "Unlike an RNN or a SimpleRNN, the internal structure of the LSTM cell is more complex.  \n",
        "\n",
        "  - LSTM uses not only the present input and past outputs or hidden states, but __it introduces a cell state, s t , that carries information from one cell to the other__. \n",
        "  - __The information flow between cell states is controlled by three gates, forget gate , input gate , and output gate__. \n",
        "  - The three gates have the effect of determining \n",
        "    - which information should be retained or replaced and \n",
        "    - the amount of information in the past and current input that should contribute to the current cell state or output.\n",
        "  - __The LSTM() layer can be used as a drop-in replacement for SimpleRNN()__. \n",
        "  - If LSTM is overkill for the task at hand, a simpler version called a Gated Recurrent Unit ( GRU ) can be used. \n",
        "    - A GRU simplifies LSTM by combining the cell state and hidden state together. \n",
        "    - A GRU also reduces the number of gates by one. \n",
        "    - __The GRU() function can also be used as a drop-in replacement for SimpleRNN()__.\n",
        "\n",
        "  - There are many other ways to configure RNNs. \n",
        "    - One way is making an RNN model that is bidirectional. By default, RNNs are unidirectional in the sense that the current output is only influenced by the past states and the current input.\n",
        "    - __In bidirectional RNNs, future states can also influence the present and past states by allowing information to flow backward.__ Past outputs are updated as needed depending on the new information received. \n",
        "      - RNNs can be made bidirectional by calling a wrapper function. \n",
        "        - For example, __the implementation of bidirectional LSTM is Bidirectional(LSTM())__ .\n",
        "\n",
        "    - __For all types of RNNs, increasing the number of units will also increase the capacity.__ \n",
        "    - __another way of increasing the capacity is by stacking the RNN layers.__\n",
        "\n",
        "    - It should be noted though that as a general rule of thumb, __the capacity of the model should only be increased if needed.__\n",
        "        - Excess capacity may contribute to overfitting, and, as a result, may lead to both a longer training time and a slower performance during prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D2Rwnwdrrib"
      },
      "source": [
        "## LSTM with mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK80GZmtrwPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07edc031-d094-420a-ccb2-6d7fffd5183b"
      },
      "source": [
        "# Parameters for LSTM network\n",
        "n_units = 30\n",
        "\n",
        "# Build LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(n_units ,\n",
        "               dropout=dropout,\n",
        "               input_shape=input_shape))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size = batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 5s 5ms/step - loss: 1.2311 - accuracy: 0.5956\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.5381 - accuracy: 0.8324\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3364 - accuracy: 0.8975\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2595 - accuracy: 0.9216\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2174 - accuracy: 0.9348\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1896 - accuracy: 0.9429\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1725 - accuracy: 0.9476\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1545 - accuracy: 0.9537\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1424 - accuracy: 0.9567\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1316 - accuracy: 0.9602\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1240 - accuracy: 0.9628\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1163 - accuracy: 0.9646\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1104 - accuracy: 0.9665\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1074 - accuracy: 0.9674\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1011 - accuracy: 0.9692\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0984 - accuracy: 0.9698\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0923 - accuracy: 0.9719\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0902 - accuracy: 0.9725\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0876 - accuracy: 0.9730\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0849 - accuracy: 0.9741\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e8a629590>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-MVG4lcvHwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9322ce5-3141-4482-c741-1b8082224bd2"
      },
      "source": [
        "# Step 5: Model evaluation\n",
        "\n",
        "__, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"Test Accuracy: %.1f%%\" %(100*acc))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 3ms/step - loss: 0.0723 - accuracy: 0.9776\n",
            "Test Accuracy: 97.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lszY947Cwscd"
      },
      "source": [
        "## GRU with mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMsXp_DTxQ8r",
        "outputId": "c4137d13-9e5d-4aed-a8c8-ba7f707d748d"
      },
      "source": [
        "# Parameters for GRU network\n",
        "n_units = 30\n",
        "\n",
        "# Build GRU network\n",
        "model = Sequential()\n",
        "model.add(GRU(n_units ,\n",
        "               dropout=dropout,\n",
        "               input_shape=input_shape))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size = batch_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 1.3460 - accuracy: 0.5351\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.6257 - accuracy: 0.7994\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3949 - accuracy: 0.8798\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3026 - accuracy: 0.9090\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2581 - accuracy: 0.9220\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2267 - accuracy: 0.9319\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2055 - accuracy: 0.9380\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1851 - accuracy: 0.9430\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1731 - accuracy: 0.9473\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1587 - accuracy: 0.9514\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1497 - accuracy: 0.9545\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1391 - accuracy: 0.9576\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1328 - accuracy: 0.9595\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1280 - accuracy: 0.9617\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1202 - accuracy: 0.9636\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1141 - accuracy: 0.9656\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1099 - accuracy: 0.9672\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1051 - accuracy: 0.9682\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1017 - accuracy: 0.9689\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0975 - accuracy: 0.9704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e96e24810>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru4eN62-zWlK",
        "outputId": "f569f557-cec0-4c2c-9d89-2bf4300708f7"
      },
      "source": [
        "# Step 5: Model evaluation\n",
        "\n",
        "__, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"Test Accuracy: %.1f%%\" %(100*acc))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 3ms/step - loss: 0.0759 - accuracy: 0.9774\n",
            "Test Accuracy: 97.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrgGX88Xz7QM"
      },
      "source": [
        "## Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHl4-C9F1AlV",
        "outputId": "f1c3e5be-6086-460a-92f1-b3e9a0ae4398"
      },
      "source": [
        "# Parameters for LSTM network\n",
        "n_units = 30\n",
        "\n",
        "# Build LSTM network\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(n_units, \n",
        "                             dropout=dropout,\n",
        "                             input_shape=input_shape), \n",
        "                        merge_mode='sum'))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size = batch_size)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 7s 8ms/step - loss: 0.9136 - accuracy: 0.7067\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3301 - accuracy: 0.8956\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2192 - accuracy: 0.9314\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1716 - accuracy: 0.9473\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1406 - accuracy: 0.9573\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1235 - accuracy: 0.9621\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1089 - accuracy: 0.9661\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0978 - accuracy: 0.9700\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0917 - accuracy: 0.9716\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0834 - accuracy: 0.9740\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0793 - accuracy: 0.9754\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0733 - accuracy: 0.9768\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0692 - accuracy: 0.9788\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0657 - accuracy: 0.9793\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0622 - accuracy: 0.9809\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0595 - accuracy: 0.9812\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0575 - accuracy: 0.9825\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0558 - accuracy: 0.9824\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.0538 - accuracy: 0.9831\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0534 - accuracy: 0.9834\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e97586710>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUPtZx2F2UXK",
        "outputId": "6f0603c0-11fc-42de-8a5c-2442b4624145"
      },
      "source": [
        "# Step 5: Model evaluation\n",
        "\n",
        "__, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"Test Accuracy: %.1f%%\" %(100*acc))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 4ms/step - loss: 0.0593 - accuracy: 0.9825\n",
            "Test Accuracy: 98.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTEWr8d-8Ap6",
        "outputId": "9e94e48f-7e9e-46e7-87d8-9c1df147172c"
      },
      "source": [
        "end = time.time()\n",
        "print('종료시간: ', time.ctime(end))\n",
        "print('경과시간(분): ', ((end-start)/60))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "종료시간:  Thu Sep 16 01:33:21 2021\n",
            "경과시간(분):  10.435800238450367\n"
          ]
        }
      ]
    }
  ]
}