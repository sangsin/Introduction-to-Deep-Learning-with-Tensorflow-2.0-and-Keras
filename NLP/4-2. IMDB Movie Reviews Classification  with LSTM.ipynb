{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-2. IMDB Movie Reviews Classification  with LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlLoCLMHMGHl"
      },
      "source": [
        "# Problem Description\n",
        "\n",
        "The problem that we will use to demonstrate sequence learning in this tutorial is the [IMDB movie review](http://ai.stanford.edu/~amaas/data/sentiment/) sentiment classification problem. \n",
        "\n",
        "  - The data was collected by [Stanford researchers and was used in a 2011 paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) where a split of 50-50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
        "  - Keras provides access to the IMDB dataset built-in. The imdb.load_data() function allows you to load the dataset.\n",
        "  - Each movie review is a variable sequence of words and the sentiment of each movie review must be classified.\n",
        "  - We will map each movie review into a real vector domain, a popular technique when working with text called word embedding. \n",
        "    - We will map each word onto a 32 length real valued vector. \n",
        "    - We will also limit the total number of words that we are interested in modeling to the 5000 most frequent words, and zero out the rest.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbN_VOMKQd_n",
        "outputId": "009d15e4-6dd8-42c9-faf6-8f952f2199a4"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "print('시작시간:', time.ctime(start))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "시작시간: Thu Sep 16 02:11:01 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e9yGgXmOKyy"
      },
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ott7cQVlO8LB"
      },
      "source": [
        "# Simple LSTM for Sequence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQzLJTCOaAD"
      },
      "source": [
        "## Step 1: Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEdT_fUyOREk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab26b22-f2d5-4f60-e4fe-56b645cbcf45"
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TzaLB66PD60"
      },
      "source": [
        "# truncate and pad input sequences\n",
        "\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vppLLVcPtiT"
      },
      "source": [
        "## Step 2 ~ 4: Build, compile and fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZJqHay9P2ZY",
        "outputId": "2b9872de-51de-4510-bd69-6d77ca1dc220"
      },
      "source": [
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))  # 100 memory units (smart neurons)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64) \n",
        "# The model is fit for only 3 epochs because it quickly overfits the problem. "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 406s 1s/step - loss: 0.4777 - accuracy: 0.7584 - val_loss: 0.3229 - val_accuracy: 0.8676\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 324s 828ms/step - loss: 0.3146 - accuracy: 0.8741 - val_loss: 0.3183 - val_accuracy: 0.8679\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 296s 757ms/step - loss: 0.2898 - accuracy: 0.8835 - val_loss: 0.3726 - val_accuracy: 0.8484\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4518f73e10>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW6RgTsoQbED"
      },
      "source": [
        "## Step 5: Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRKri_pyQWj4",
        "outputId": "862440ea-361b-4584-c1ab-b894183d1597"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 84.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-P6WM4JQkai"
      },
      "source": [
        "You can see that this simple LSTM with little tuning achieves near [state-of-the-art results](https://paperswithcode.com/sota/sentiment-analysis-on-imdb) on the IMDB problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA446TK6UxeS"
      },
      "source": [
        "# LSTM For Sequence Classification With Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4b_ImOxWSDG",
        "outputId": "67b8a91a-3890-4eb3-fe8b-fbbc15372d4b"
      },
      "source": [
        "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))   # newly added\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))   # newly added\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64,validation_split=0.2)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "313/313 [==============================] - 206s 652ms/step - loss: 0.4990 - accuracy: 0.7422 - val_loss: 0.3485 - val_accuracy: 0.8548\n",
            "Epoch 2/3\n",
            "313/313 [==============================] - 207s 660ms/step - loss: 0.2964 - accuracy: 0.8778 - val_loss: 0.3326 - val_accuracy: 0.8676\n",
            "Epoch 3/3\n",
            "313/313 [==============================] - 207s 662ms/step - loss: 0.2460 - accuracy: 0.9045 - val_loss: 0.3482 - val_accuracy: 0.8732\n",
            "782/782 [==============================] - 79s 101ms/step - loss: 0.3637 - accuracy: 0.8672\n",
            "Accuracy: 86.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObLENxk_Wtr3"
      },
      "source": [
        "We can see dropout having the desired impact on training with a slightly slower trend in convergence and in this case a lower final accuracy. The model could probably use a few more epochs of training and may achieve a higher skill (try it an see)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iblXI5hfb9EE"
      },
      "source": [
        "# LSTM For Sequence Classification With [Dropout and Recurrent Dropout](https://stackoverflow.com/questions/49940280/keras-lstm-dropout-vs-recurrent-dropout)\n",
        "\n",
        "  - Dropout can be applied to the input and recurrent connections of the memory units with the LSTM precisely and separately.\n",
        "  - Keras provides this capability with parameters on the LSTM layer, the dropout for configuring the input dropout and recurrent_dropout for configuring the recurrent dropout\n",
        "  - Recurrent Dropout is a regularization method for recurrent neural networks. Dropout is applied to the updates to LSTM memory cells (or GRU states), i.e. it drops out the input/update gate in LSTM/GRU.\n",
        "\n",
        "  >\n",
        "  > dropout: [0,1], Fraction of the units to drop for the linear transformation of the inputs.\n",
        "  >\n",
        "  > recurrent_dropout: [0,1],  Fraction of the units to drop for the linear transformation of the recurrent state.\n",
        "  >\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj1btvNycrK5",
        "outputId": "98eab334-c98c-4dd7-e9e8-cd3d4bb4c180"
      },
      "source": [
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # newly modified\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 512s 1s/step - loss: 0.4793 - accuracy: 0.7671\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 523s 1s/step - loss: 0.3222 - accuracy: 0.8685\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 594s 2s/step - loss: 0.2520 - accuracy: 0.9004\n",
            "Accuracy: 87.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdj8N-7ZgX9y"
      },
      "source": [
        "We can see that the LSTM specific dropout has a more pronounced effect on the convergence of the network than the layer-wise dropout. \n",
        "\n",
        "As above, the number of epochs was kept constant and could be increased to see if the skill of the model can be further lifted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFTPEMq5m6-E"
      },
      "source": [
        "# LSTM and Convolutional Neural Network For Sequence Classification\n",
        "\n",
        "  - Convolutional neural networks excel at learning the spatial structure in input data.\n",
        "  - The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.\n",
        "  - We can easily add a one-dimensional CNN and max pooling layers after the Embedding layer which then feed the consolidated features to the LSTM. \n",
        "    - We can use a smallish set of 32 features with a small filter length of 3. \n",
        "    - The pooling layer can use the standard length of 2 to halve the feature map size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03xIMYtxngK_",
        "outputId": "7b3ee1e3-2f8a-480f-9d15-b2f54b7def96"
      },
      "source": [
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))   # newly added\n",
        "model.add(MaxPooling1D(pool_size=2))    # newly added\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 216,405\n",
            "Trainable params: 216,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 175s 438ms/step - loss: 0.4236 - accuracy: 0.7943\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 176s 451ms/step - loss: 0.2480 - accuracy: 0.9043\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 180s 459ms/step - loss: 0.2056 - accuracy: 0.9220\n",
            "Accuracy: 88.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Ofp_V4n3qh"
      },
      "source": [
        "We can see that we achieve similar results to the first example although with faster training time.\n",
        "\n",
        "I would expect that even better results could be achieved if this example was further extended to use dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLCwQ3geMKbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38584c9c-2115-4af7-bb01-b87159b08a5e"
      },
      "source": [
        "execution_time = time.time() - start\n",
        "print('실행시간(분): ', execution_time/60)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실행시간(분):  70.17539571126302\n"
          ]
        }
      ]
    }
  ]
}