{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-6. BERT vs GPT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRgVa7SkclLa"
      },
      "source": [
        "# BERT vs GPT\n",
        "\n",
        "참고\n",
        "  1. [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)\n",
        "  2. [BERT와 GPT](https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- GPT(Generative Pre-trained Transformer)는 전통적인 언어모델(Language Model. LM)\n",
        "  - 이전 단어들이 주어졌을 때 다음 단어가 무엇인지 맞추는 과정으로 프리트레인(pretrain)\n",
        "  - 문장 시작부터 순차적으로 계산한다는 점에서 일방향(unidirectional)입니다.\n",
        "\n",
        "  <img src='https://jalammar.github.io/images/xlnet/gpt-2-output.gif'>\n",
        "  -  트랜스포머에서 인코더를 제외하고 디코더만 사용\n",
        "\n",
        "  <img src= 'https://i.imgur.com/Q7IS78n.png'>\n",
        "\n",
        "- BERT(Bidirectional Encoder Representations from Transformers)는 마스크 언어모델(Masked Language Model, MLM)\n",
        "  - 문장 중간에 빈칸을 만들고 해당 빈칸에 어떤 단어가 적절할지 맞추는 과정으로 프리트레인 \n",
        "  - 빈칸 앞뒤 문맥을 모두 살필 수 있다는 점에서 양방향(bidirectional) 성격\n",
        "  -트랜스포머에서 디코더를 제외하고 인코더만 사용\n",
        "\n",
        "    <img src='https://i.imgur.com/ekmm63h.png'>\n",
        "\n",
        "  - 이 때문에 GPT는 문장 생성에, BERT는 문장의 의미를 추출하는 데 강점\n",
        "\n",
        "  <img src='https://i.imgur.com/S0equuG.png'>\n",
        "  <img src='https://blog.floydhub.com/content/images/2020/02/bert-vs-gpt-1.png'>\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq5mIpfeiTre"
      },
      "source": [
        "# 최근 트렌트: Beyond Transformer\n",
        "\n",
        "  - 우선 모델 크기 증가 추세가 눈에 띕니다. \n",
        "    - 아래 표를 보면 파라메터 수 기준 GPT3(2020)는 GPT1 대비 1400배, GPT2 대비 117배 증가했습니다. \n",
        "    - OpenAI에 따르면 모델 크기 증가는 언어모델 품질은 물론 각종 다운스트림 태스크의 성능 개선에 큰 도움이 된다고 합니다.\n",
        "\n",
        "|모델명\t|사이즈|\n",
        "|---|---|\n",
        "|GPT1\t|0.125B(=125M)|\n",
        "|GPT2|\t1.5B|\n",
        "|GPT3|\t175B|\n",
        "\n",
        "  - 이와 별개로 모델 성능을 최대한 유지하면서 크기를 줄이려는 시도도 계속되고 있습니다(그림12). \n",
        "    - 디스틸레이션(Distillation), 퀀타이제이션(Quantization), 프루닝(Pruning), 파라메터 공유(Weight Sharing) 등\n",
        "\n",
        "  - 트랜스포머 아키텍처에서 벗어나 새로운 구조의 언어모델도 고안되고 있습니다. \n",
        "    - 일렉트라(electra)\n",
        "\n",
        "\n",
        "[Beyond BERT](http://dmqm.korea.ac.kr/uploads/seminar/Beyond%20Bert.pdf)\n",
        "\n",
        "- XLNet : Transformer의 디코더 구조를 쌓아올려 구성된 모델로, BERT의 한계점을 개선시키고자 Autoencoding + AutoRegressive 구조를 제안한 모델이다.\n",
        "- SpanBERT : BERT의 pre-training 방식을 개선시키기 위하여 연속된 시퀀스 내의 모든 토큰을 masking하는 방법을 제안하였다.\n",
        "- RoBERTa : BERT를 최적화시키기 위하여 더 많은 양의 데이터와 배치 사이즈를 활용하여 pre-train 시킨 모델이다.\n",
        "- ALBERT : BERT 모델의 효율화를 진행한 방법으로, 임베딩의 추가나 Transformer encoder에서의 파라미터를 공유하는 등의 방식을 적용하였다.\n",
        "- BART : Transformer의 구조를 거의 그대로 활용하여 새로운 Autoencoding + AutoRegressive 구조를 제안하였다.\n",
        "- ELECTRA : GAN과 유사한 구조를 제안하여 mask된 토큰을 맞추는 부분과, 새롭게 생성된 토큰이 원본 문장과 동일한지 비교하는 부분으로 구성된 모델이다.\n",
        "- DeBERTa : 한 토큰을 두 개의 임베딩 값으로 표현하여 계산하는 Disentangled attention mechanism을 활용하여 단어의 상대적인 위치도 잘 고려한 모델로, 현재 SOTA 모델로 알려져있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAMzREaqqZ4L"
      },
      "source": [
        "# [GPT-3 패러다임을 바꿀 미친 성능의 인공지능 등장 및 활용 사례 10가지](https://www.youtube.com/watch?v=I7sZVrwM6_Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwt67UTuv0O_"
      },
      "source": [
        "# [엄~청 큰 언어 모델 공장 가동기!](https://deview.kr/data/deview/2019/presentation/[111]+%E1%84%8B%E1%85%A5%E1%86%B7_%E1%84%8E%E1%85%A5%E1%86%BC+%E1%84%8F%E1%85%B3%E1%86%AB+%E1%84%8B%E1%85%A5%E1%86%AB%E1%84%8B%E1%85%A5+%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF+%E1%84%80%E1%85%A9%E1%86%BC%E1%84%8C%E1%85%A1%E1%86%BC+%E1%84%80%E1%85%A1%E1%84%83%E1%85%A9%E1%86%BC%E1%84%80%E1%85%B5.pdf)"
      ]
    }
  ]
}