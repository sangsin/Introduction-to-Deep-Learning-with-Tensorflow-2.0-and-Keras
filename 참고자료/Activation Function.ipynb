{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "__What is Activation Function?__\n",
    "\n",
    "It’s just a thing function that you use to get the output of node. It is also known as Transfer Function.\n",
    "\n",
    "### 1. Sigmoid or Logistic Activation Function\n",
    "\n",
    "The Sigmoid Function curve looks like a S-shape.\n",
    "\n",
    "$$ f(x) = \\frac{1}{1+e^{-x}} = \\frac {e^x}{e^x+1} = \\frac {1}{2} + \\frac {1}{2}tanh( \\frac {x}{2}) $$\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/728/1*Xu7B5y9gp0iL5ooBj7LtWw.png\">\n",
    "\n",
    "- The range of the sigmoid function is from (-1 to 1).\n",
    "- Therefore, it is especially __used for models where we have to predict the probability as an output.__\n",
    "- The function is differentiable.\n",
    "- The function is monotonic but function’s derivative is not.\n",
    "- The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
    "- The softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
    "\n",
    "$$ f(x_i) = \\frac {e^{x_i}}{\\sum_{i}{e^{x_i}}}$$\n",
    "\n",
    "### 2. Tanh or hyperbolic tangent Activation Function\n",
    "\n",
    "tanh is also sigmoidal (s - shaped).\n",
    "\n",
    "$$ tanh(x) = \\frac {sinh(x)}{cosh(x)} = \\frac {e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/893/1*f9erByySVjTjohfFdNkJYQ.jpeg\">\n",
    "\n",
    "- The range of the tanh function is from (-1 to 1). \n",
    "- The function is differentiable.\n",
    "- The function is monotonic while its derivative is not monotonic.\n",
    "- The tanh function is mainly used classification between two classes.\n",
    "- Both tanh and logistic sigmoid activation functions are used in feed-forward nets.\n",
    "\n",
    "### 3. ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "$$ R(z) =  max(z,0) = \\begin{cases} \n",
    "                        0 & \\mbox{for } z < 0\\\\\n",
    "                        z & \\mbox {for } z \\ge 0   \n",
    "             \\end{cases} $$ \n",
    " \n",
    "                    \n",
    "<img src = \"https://miro.medium.com/max/1050/1*XxxiA0jJvPrHEJHD4z893g.png\">\n",
    "\n",
    "\n",
    "- It is __used in almost all the convolutional neural networks.\n",
    "- Range: [0, $\\infty$]\n",
    "- The function and its derivative both are monotonic.\n",
    "- But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly.\n",
    "\n",
    "### 4. Leaky ReLU\n",
    "\n",
    "It is an attempt to solve the dying ReLU problem\n",
    "\n",
    "$$ f(x) =  max(ax,x) = \\begin{cases} \n",
    "                        ax & \\mbox{for } x < 0\\\\\n",
    "                        x & \\mbox {for } x \\ge 0   \n",
    "             \\end{cases} $$ \n",
    "\n",
    "The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so\n",
    "When $a$ is not 0.01 then it is called __Randomized ReLU__.\n",
    "\n",
    "<img src = \"https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/09/Deep-learning-25-i2tutorials.png\">\n",
    "\n",
    "- Range: [ -$\\infty$, $\\infty$ ]\n",
    "- Both Leaky and Randomized ReLU functions are monotonic. \n",
    "- Also, their derivatives also monotonic in nature.\n",
    "\n",
    "\n",
    "### Why derivative/differentiation is used ?\n",
    "\n",
    "When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.That is why we use differentiation in almost every part of Machine Learning and Deep Learning.\n",
    "\n",
    "<img src= \"https://miro.medium.com/max/1050/1*p_hyqAtyI8pbt2kEl6siOQ.png\">\n",
    "\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1050/1*n1HFBpwv21FCAzGjmWt1sg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "v= [1,-3,3, 0, -0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__sigmoid or logistic__ activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73105858 0.04742587 0.95257413 0.5        0.37754067]\n"
     ]
    }
   ],
   "source": [
    "# calculate the sigmoid of a vector\n",
    "\n",
    "def np_sigmoid(x):\n",
    "    s = 1/(1+1/np.exp(x))\n",
    "    return  s\n",
    "\n",
    "print(np_sigmoid(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__tanh or hyperboic tangent__ activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76159416 -0.99505475  0.99505475  0.         -0.46211716]\n"
     ]
    }
   ],
   "source": [
    "def tanh(x):\n",
    "    s = np.tanh(x)\n",
    "    return  s\n",
    "\n",
    "print(tanh(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ReLU__ activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 3. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    s = np.maximum(x,0)\n",
    "    return s\n",
    "\n",
    "print(relu(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leaky ReLU__ activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.    -0.03   3.     0.    -0.005]\n"
     ]
    }
   ],
   "source": [
    "def leaky_ReLU(x):\n",
    "    s = np.where(x > 0, x, x * 0.01)     \n",
    "    return s\n",
    "\n",
    "vv = np.array(v)\n",
    "print (leaky_ReLU(vv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__softmax__ activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.66524096 0.24472847]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# calculate the softmax of a vector\n",
    "def softmax(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / e.sum()\n",
    " \n",
    "# define data\n",
    "data = [1, 3, 2]\n",
    "# convert list of numbers to a list of probabilities\n",
    "result = softmax(data)\n",
    "# report the probabilities\n",
    "print(result)\n",
    "# report the sum of the probabilities\n",
    "print(sum(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
