{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Tensorflow (GPU)",
      "language": "python",
      "name": "py3.6-tfgpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "MNIST DNN Classification in Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg7eCV3e9JQy"
      },
      "source": [
        "# Typical architecture of a classification neural network \n",
        "\n",
        "The word *typical* is on purpose.\n",
        "\n",
        "Because the architecture of a classification neural network can widely vary depending on the problem you're working on.\n",
        "\n",
        "However, there are some fundamentals all deep neural networks contain:\n",
        "* An input layer.\n",
        "* Some hidden layers.\n",
        "* An output layer.\n",
        "\n",
        "Much of the rest is up to the data analyst creating the model.\n",
        "\n",
        "The following are some standard values you'll often use in your classification neural networks.\n",
        "\n",
        "| **Hyperparameter** | **Binary Classification** | **Multiclass classification** |\n",
        "| --- | --- | --- |\n",
        "| Input layer shape | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) | Same as binary classification |\n",
        "| Hidden layer(s) | Problem specific, minimum = 1, maximum = unlimited | Same as binary classification |\n",
        "| Neurons per hidden layer | Problem specific, generally 10 to 100 | Same as binary classification |\n",
        "| Output layer shape | 1 (one class or the other) | 1 per class (e.g. 3 for food, person or dog photo) |\n",
        "| Hidden activation | Usually [ReLU](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) (rectified linear unit) | Same as binary classification |\n",
        "| Output activation | [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) | [Softmax](https://en.wikipedia.org/wiki/Softmax_function) |\n",
        "| Loss function | [Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) ([`tf.keras.losses.BinaryCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) in TensorFlow) | Cross entropy ([`tf.keras.losses.CategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) in TensorFlow) |\n",
        "| Optimizer | [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) (stochastic gradient descent), [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) | Same as binary classification |\n",
        "\n",
        "***Table 1:*** *Typical architecture of a classification network.* ***Source:*** *Adapted from page 295 of [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow Book by Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*\n",
        "\n",
        "[epub](http://library.lol/main/E585D7EA43B3477E96A53A0BC1220F37)\n",
        "\n",
        "[pdf](http://library.lol/main/40CA3F6E08498377145117D8B48BFD1B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfdfaXdYW3w9"
      },
      "source": [
        "## Prerequisite Python Modules\n",
        "\n",
        "First, some software needs to be loaded into the Python environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGdJT9cXvg5r",
        "outputId": "d6cae432-7601-46c0-f7fe-16ec48ca4731"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np                   # advanced math library\n",
        "import matplotlib.pyplot as plt      # MATLAB like plotting routines\n",
        "import random                        # for generating random numbers\n",
        "\n",
        "from tensorflow.keras.models import Sequential  # Model type to be used\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Flatten, Dropout, Activation\n",
        "from tensorflow.keras.datasets import mnist     # MNIST dataset is included in Keras\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "\n",
        "print(tf.__version__) # find the version number (should be 2.x+)\n",
        "\n",
        "# 그래피카드 유무 확인 및 메모리 확장 설정\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  print('사용가능한 GPU 갯수: ',len(gpus), '\\n')\n",
        "      \n",
        "  try:\n",
        "    # 프로그램이 실행되어 더 많은 GPU 메모리가 필요하면, 텐서플로 프로세스에 할당된 GPU 메모리 \n",
        "    # 영역을 확장할 수있도록 허용\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "  except RuntimeError as e:\n",
        "    # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
        "    print(e)\n",
        "\n",
        "# 설치된 GPU 상세내용 확인\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n",
            "사용가능한 GPU 갯수:  1 \n",
            "\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15019991776750708144\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 16183459840\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 16480111451695820662\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXCKUiQ5W3w9"
      },
      "source": [
        "## Loading Training Data\n",
        "\n",
        "The MNIST dataset is conveniently bundled within Keras, and we can easily analyze some of its features in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puaI5Ng9W3w9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b984ef6-6d4a-4344-b457-09bbaac680eb"
      },
      "source": [
        "# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"X_train shape\", X_train.shape)\n",
        "print(\"y_train shape\", y_train.shape)\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"y_test shape\", y_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "X_train shape (60000, 28, 28)\n",
            "y_train shape (60000,)\n",
            "X_test shape (10000, 28, 28)\n",
            "y_test shape (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBKogzYeW3w-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6c7617-6106-4ae1-bc37-6832cd240a49"
      },
      "source": [
        "X_train = X_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.\n",
        "X_test = X_test.reshape(10000, 784)   # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.\n",
        "\n",
        "X_train = X_train.astype('float32')   # change integers to 32-bit floating point numbers\n",
        "X_test = X_test.astype('float32')\n",
        "print\n",
        "X_train /= 255                        # normalize each value for each pixel for the entire vector for each input\n",
        "X_test /= 255\n",
        "\n",
        "print(\"Training matrix shape\", X_train.shape)\n",
        "print(\"Testing matrix shape\", X_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training matrix shape (60000, 784)\n",
            "Testing matrix shape (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsPhNJS_W3w-"
      },
      "source": [
        "We then modify our classes (unique digits) to be in the **one-hot format**, i.e.\n",
        "\n",
        "```\n",
        "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "etc.\n",
        "```\n",
        "\n",
        "If the final output of our network is very close to one of these classes, then it is most likely that class. For example, if the final output is:\n",
        "\n",
        "```\n",
        "[0, 0.94, 0, 0, 0, 0, 0.06, 0, 0, 0]\n",
        "```\n",
        "then it is most probable that the image is that of the digit `1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "LJ3dLcpOW3w_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69320f76-4dbd-4742-ad12-ba147cf20601"
      },
      "source": [
        "X_test[0]\n",
        "nb_classes = 10 # number of unique digits\n",
        "\n",
        "Y_train = to_categorical(y_train,nb_classes)\n",
        "Y_test = to_categorical(y_test,nb_classes)\n",
        "X_test[0]\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
              "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
              "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
              "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
              "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
              "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
              "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
              "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
              "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
              "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
              "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
              "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
              "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
              "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
              "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
              "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
              "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
              "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_1GgNcnW3w_"
      },
      "source": [
        "# Building a 3-layer fully connected network (FCN)\n",
        "\n",
        "<img src=\"https://github.com/AviatorMoser/keras-mnist-tutorial/blob/master/figure.png?raw=1\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJFsrjvW3w_"
      },
      "source": [
        "## A DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iK3wdWhKW3w_"
      },
      "source": [
        "# The Sequential model is a linear stack of layers and is very common.\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# The first hidden layer is a set of 512 nodes (artificial neurons).\n",
        "# Each node will receive an element from each input vector and apply some weight and bias to it.\n",
        "\n",
        "model.add(Dense(512, input_shape=(784,))) #(784,) is not a typo -- that represents a 784 length vector!\n",
        "model.add(Activation('relu')) # # An \"activation\" is a non-linear function applied to the output of the layer.\n",
        "                              # The Rectified Linear Unit (ReLU) converts all negative inputs to nodes in the next layer to be zero.\n",
        "                              # Those inputs are then not considered to be fired.\n",
        "                              # Positive values of a node are unchanged.\n",
        "model.add(Dropout(0.2)) # Dropout zeroes a selection of random outputs (i.e., disables their activation)\n",
        "                        # Dropout helps protect the model from memorizing or \"overfitting\" the training data.\n",
        "\n",
        "# The second hidden layer appears identical to our first layer.\n",
        "# However, instead of each of the 512-node receiving 784-inputs from the input image data,\n",
        "# they receive 512 inputs from the output of the first 512-node layer.\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# The final layer of 10 neurons in fully-connected to the previous 512-node layer.\n",
        "# The final layer of a FCN should be equal to the number of desired classes (10 in this case).\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax')) # The \"softmax\" activation represents a probability distribution over K different possible outcomes.\n",
        "                                 # Its values are all non-negative and sum to 1."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdfVaDonW3xA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ed5ef6-2f24-4ede-8e36-92b861271d49"
      },
      "source": [
        "# Summarize the built model\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK5-5Z5kW3xA"
      },
      "source": [
        "## Compiling the model\n",
        "\n",
        "Keras is built on top of Theano and TensorFlow. Both packages allow you to define a *computation graph* in Python, which then compiles and runs efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
        "\n",
        "When compiing a model, Keras asks you to specify your **loss function** and your **optimizer**. The loss function we'll use here is called *categorical cross-entropy*, and is a loss function well-suited to comparing two probability distributions.\n",
        "\n",
        "Our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
        "\n",
        "The optimizer helps determine how quickly the model learns through **gradient descent**. The rate at which descends a gradient is called the **learning rate**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd09XePUW3xB"
      },
      "source": [
        "<img src = \"https://randlow.github.io/images/ml/gradient-descent.png\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blv2zCyJW3xB"
      },
      "source": [
        "<img src = \"https://srdas.github.io/DLBook/DL_images/TNN2.png\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnYY6BszW3xB"
      },
      "source": [
        "So are smaller learning rates better? Not quite! It's important for an optimizer not to get stuck in local minima while neglecting the global minimum of the loss function. Sometimes that means trying a larger learning rate to jump out of a local minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W57w2l-CW3xB"
      },
      "source": [
        "<img src = 'https://miro.medium.com/max/2060/1*9DVEXY4X0eNAx_ZHoD_PPA.png' >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rHAeFC6aW3xB"
      },
      "source": [
        "# Let's use the Adam optimizer for learning\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX_FHQaRW3xB"
      },
      "source": [
        "## Train the model!\n",
        "This is the fun part! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXeZbVKOW3xB"
      },
      "source": [
        "The batch size determines over how much data per step is used to compute the loss function, gradients, and back propagation. Large batch sizes allow the network to complete it's training faster; however, there are other factors beyond training speed to consider.\n",
        "\n",
        "Too large of a batch size smoothes the local minima of the loss function, causing the optimizer to settle in one because it thinks it found the global minimum.\n",
        "\n",
        "Too small of a batch size creates a very noisy loss function, and the optimizer may never find the global minimum.\n",
        "\n",
        "So **a good batch size may take some trial and error to find!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKn2ZuocW3xB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fac0a7-d259-4341-c0e8-32e1c9d2c914"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "history = model.fit(X_train, Y_train, batch_size=128, epochs=10,\n",
        "           validation_data=(X_test, Y_test), verbose=1)\n",
        "end = time.time()\n",
        "print('Execution time in seconds =',end-start)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.1145 - val_accuracy: 0.9822\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.0853 - val_accuracy: 0.9857\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0114 - accuracy: 0.9968 - val_loss: 0.0813 - val_accuracy: 0.9864\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0116 - accuracy: 0.9963 - val_loss: 0.0814 - val_accuracy: 0.9859\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.1043 - val_accuracy: 0.9839\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.0905 - val_accuracy: 0.9846\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 0.0817 - val_accuracy: 0.9849\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0096 - accuracy: 0.9968 - val_loss: 0.0846 - val_accuracy: 0.9866\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 0.0928 - val_accuracy: 0.9845\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0876 - val_accuracy: 0.9858\n",
            "Execution time in seconds = 13.191210746765137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "TuAraeeB-0dx",
        "outputId": "600299ff-fe3c-4760-aa49-560a40caa64b"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(history.history).plot(title=\"Loss and Accuracy\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff955e0af50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNl_V09KW3xB"
      },
      "source": [
        "The two numbers, in order, represent the value of the loss function of the network on the training set, and the overall accuracy of the network on the training data. But how does it do on data it did not train on?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-YrkSLU7wqY"
      },
      "source": [
        "## Check the accuracy for test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzB4So0M7jJZ",
        "outputId": "a64fd9a2-ad37-48d3-855a-f6fa9c292615"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print('Test score:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0876 - accuracy: 0.9858\n",
            "Test score: 0.08761416375637054\n",
            "Test accuracy: 0.98580002784729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G4TRpyxW3xC"
      },
      "source": [
        "## Evaluate Model's Accuracy on Test Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APcfa1DHWtqR"
      },
      "source": [
        "### 분류성능평가지표 - 혼동행렬(Confusion Matrix)\n",
        "\n",
        "모델을 평가하는 요소는 결국, 모델이 내놓은 답과 실제 정답의 관계로써 정의를 내릴 수 있습니다. 정답이 True와 False로 나누어져있고, 분류 모델 또한 True False의 답을 내놓습니다. 그렇게 하면, 이진분류(Binary Classification)의 경우 아래와 같이 2x2 matrix로 case를 나누어볼 수 있습니다.\n",
        "\n",
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/99DC064C5BE056CE10\">\n",
        "\n",
        "위와 같이 구분한 표를 혼동행렬(Confusion Matrix)이라고 합니다\n",
        "```\n",
        "  - True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)\n",
        "  - False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)\n",
        "  - False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)\n",
        "  - True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)\n",
        "```\n",
        "\n",
        "  - False Positives (FP): We predicted yes, but they don't actually have the disease.\n",
        "  - False Negatives (FN): We predicted no, but they actually do have the disease. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO_EjFDwUPkc"
      },
      "source": [
        "###  분류성능평가지표 - Precision(정밀도)과 Recall(재현율)\n",
        "\n",
        "  - Precision(정밀도): 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율\n",
        "\n",
        "    $$Precision =  \\frac {TP}{TP+FP}$$\n",
        "\n",
        "  - Recall(재현율): 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율\n",
        "\n",
        "    $$Recall =  \\frac {TP}{TP+FN}$$\n",
        "\n",
        "    통계학에서는 sensitivity로, 그리고 다른 분야에서는 hit rate라는 용어로도 사용합니다. 실제 날씨가 맑은 날 중에서 모델이 맑다고 예측한 비율을 나타낸 지표인데, 정밀도(Precision)와 True Positive의 경우를 다르게 바라보는 것입니다. 즉, Precision이나 Recall은 모두 실제 True인 정답을 모델이 True라고 예측한 경우에 관심이 있으나, 바라보고자 하는 관점만 다릅니다. Precision은 모델의 입장에서, 그리고 Recall은 실제 정답(data)의 입장에서 정답을 정답이라고 맞춘 경우를 바라보고 있습니다. \n",
        "\n",
        "      <img src= \"https://t1.daumcdn.net/cfile/tistory/999D9C465BE116E43C\">\n",
        "\n",
        "  A는 실제 날씨가 맑은 날입니다. 그리고 B는 모델에서 날씨가 맑은 날이라고 예측한 것입니다.\n",
        "  $$Precison =\\frac{b}{b+c},  \\hspace{2ex}  Recall = \\frac{b}{a+b}$$\n",
        "\n",
        "  모델의 입장에서 모두 맑은 날이라고만 예측하는 경우를 생각해봅시다. 그렇게 되면 TN(d)의 영역이 줄어들게 되고 그에 따라 FN(a)의 영역 또한 줄게 됩니다. 그러므로 Recall은 분모의 일부인 FN(a)영역이 줄기 때문에 Recall은 100%가 됩니다. \n",
        "\n",
        "    <img src = \"https://t1.daumcdn.net/cfile/tistory/9951F44B5BE1205F1E\">\n",
        "\n",
        "  위 그림의 왼쪽 Case에서 Recall은 20 / 50 = 40%, Precision = 20 / 60 = 33.3% 입니다. 그리고 분류모델이 모두 True라고 예측한 오른쪽의 case에서의 recall은 FN = 0이므로 100%이지만 그에 따라 FP가 늘어서 precision은 20/100 = 20%가 되었습니다. 이처럼 precision과 recall은 모두 높은 것이 좋지만, trade-off 관계에 있어서 함께 늘리기가 힘듭니다.\n",
        "\n",
        " Precision과 Recall은 trade-off관계이며 상호보완적으로 해석해야하며, 두 지표가 모두 높을 수록 좋은 모델입니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgZcTNLcXCPU"
      },
      "source": [
        "###  분류성능평가지표 - Accuracy(정확도) 와 F1 Score\n",
        "\n",
        "  - Accuracy(정확도)\n",
        "\n",
        "  $$Accuracy(정확도) = \\frac {TP + TN}{TP+FN+FP+FN}$$\n",
        "\n",
        "  앞서 설명한 두 지표는 모두 True를 True라고 옳게 예측한 경우에 대해서만 다루었습니다. 하지만, False를 False라고 예측한 경우도 옳은 경우입니다. 이때, 해당 경우를 고려하는 지표가 바로 정확도(Accuracy)입니다. \n",
        "\n",
        "  정확도는 **가장 직관적으로 모델의 성능을 나타낼 수 있는 평가 지표**입니다. \n",
        "  \n",
        "  하지만, 여기서 고려해야하는 것이 있습니다. 바로 domain의 편중(bias)입니다. 만약 우리가 예측하고자 하는 한달 동안이 특정 기후에 부합하여 비오는 날이 흔치 않다고 생각해보죠. 이 경우에는 해당 data의 domain이 불균형하게되므로 맑은 것을 예측하는 성능은 높지만, 비가 오는 것을 예측하는 성능은 매우 낮을 수 밖에 없습니다. 따라서 이를 보완할 지표가 필요합니다.\n",
        "\n",
        "  - F1 Score: Precision과 Recall의 [조화평균(역수의 산술 평균의 역수)](https://ko.wikipedia.org/wiki/%EC%A1%B0%ED%99%94_%ED%8F%89%EA%B7%A0)입니다. \n",
        "\n",
        " $$F1  = 2 \\times \\frac {1}{\\frac{1}{Precision} + \\frac{1}{recall}}  = 2 \\times \\frac{Precision \\times Recall}{Precision+Recall}$$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMU2xHghkYGg"
      },
      "source": [
        "\n",
        "\n",
        "| **Metric name/Evaluation method** | **Defintion** | **Code** |\n",
        "| --- | --- | --- |\n",
        "| Accuracy | Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. | [`sklearn.metrics.accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) or [`tf.keras.metrics.Accuracy()`](tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy) |\n",
        "| Precision | Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). | [`sklearn.metrics.precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) or [`tf.keras.metrics.Precision()`](tensorflow.org/api_docs/python/tf/keras/metrics/Precision) |\n",
        "| Recall | Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. | [`sklearn.metrics.recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) or [`tf.keras.metrics.Recall()`](tensorflow.org/api_docs/python/tf/keras/metrics/Recall) |\n",
        "| F1-score | Combines precision and recall into one metric. 1 is best, 0 is worst. | [`sklearn.metrics.f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) |\n",
        "| [Confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)  | Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). | Custom function or [`sklearn.metrics.plot_confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Qio7XfkN5D"
      },
      "source": [
        "Let's start with accuracy.\n",
        "\n",
        "Because we passed `[\"accuracy\"]` to the `metrics` parameter when we compiled our model, calling `evaluate()` on it will return the loss as well as accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMmxtXIGW3xC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c303d3-f077-409b-eff8-d03b9fa248b3"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print('Test score:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0876 - accuracy: 0.9858\n",
            "Test score: 0.08761416375637054\n",
            "Test accuracy: 0.98580002784729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVsLSjMk9Xz0"
      },
      "source": [
        "# Note: The following confusion matrix code is a remix of Scikit-Learn's \n",
        "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
        "# and Made with ML's introductory notebook - https://github.com/madewithml/basics/blob/master/notebooks/09_Multilayer_Perceptrons/09_TF_Multilayer_Perceptrons.ipynb\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15): \n",
        "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
        "\n",
        "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
        "  will be used.\n",
        "\n",
        "  Args:\n",
        "    y_true: Array of truth labels (must be same shape as y_pred).\n",
        "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
        "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
        "    figsize: Size of output figure (default=(10, 10)).\n",
        "    text_size: Size of output figure text (default=15).\n",
        "  \n",
        "  Returns:\n",
        "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
        "\n",
        "  Example usage:\n",
        "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
        "                          y_pred=y_preds, # predicted labels\n",
        "                          classes=class_names, # array of class label names\n",
        "                          figsize=(15, 15),\n",
        "                          text_size=10)\n",
        "  \"\"\"  \n",
        "  # Create the confustion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "\n",
        "  # Plot the figure and make it pretty\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  # Are there a list of classes?\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "  \n",
        "  # Label the axes\n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes), \n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "  \n",
        "  # Make x-axis labels appear on bottom\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  # Set the threshold for different colors\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  # Plot the text on each cell\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "             size=text_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjIxEGCp9i8u"
      },
      "source": [
        "# Get predicted class for X_test by converting all of the predictions from probabilities to labels\n",
        "y_probs = model.predict(X_test)\n",
        "#print(tf.argmax(pred[0:10]))\n",
        "print(y_probs[0:10])\n",
        "\n",
        "y_pred=y_probs.argmax(axis=1)\n",
        "print(y_pred[0:10])\n",
        "print(y_test[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbGgosGlDEY4"
      },
      "source": [
        "# Check out the non-prettified confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true=y_test, \n",
        "                 y_pred=y_pred)\n",
        "df_cm = pd.DataFrame(cm)\n",
        "df_cm['sum']=df_cm.sum(axis=1)\n",
        "df_cm.loc['Total']= df_cm.sum()\n",
        "df_cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKXwEdvbDnQt"
      },
      "source": [
        "That confusion matrix is hard to comprehend, let's make it prettier using the function we created before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzeNOPtFDqny"
      },
      "source": [
        "# Make a prettier confusion matrix\n",
        "\n",
        "class_names=[str(i) for i in range(0,10)]\n",
        "make_confusion_matrix(y_true=y_test, \n",
        "                      y_pred=y_pred,\n",
        "                      classes=class_names,\n",
        "                      figsize=(15, 15),\n",
        "                      text_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxKguca2TXC9"
      },
      "source": [
        "### Precision, Recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DujuqnrpTj9d"
      },
      "source": [
        "# Let's consider the classification results for digit '1',\n",
        "\n",
        "TP  = df_cm.iat[0,0]\n",
        "FN = df_cm.iat[0, 10]-TP\n",
        "FP = df_cm.iat[10, 0]-TP\n",
        "\n",
        "precision = TP/(TP +FP)\n",
        "recall = TP/(TP +FN)\n",
        "F1 = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "print(\"True Positive = {},  False Negative =  {},  False Positive = {}\".format(TP,FN,FP))\n",
        "print(\"\\nPrecision = {:.4f},  Recall =  {:.4f},  F1 Score = {:.4f}\".format(precision,recall,F1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p8m0yZ6N338"
      },
      "source": [
        "# Use sklearn classification_report.\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyFVvoFaW3xD"
      },
      "source": [
        "### Inspecting the output\n",
        "\n",
        "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jKHkehhqW3xD"
      },
      "source": [
        "# The predict_classes function outputs the highest probability class\n",
        "# according to the trained classifier for each input example.\n",
        "predicted_classes = model.predict_classes(X_test)\n",
        "\n",
        "# Check which items we got right / wrong\n",
        "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
        "\n",
        "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n",
        "\n",
        "\n",
        "incorrect_indices.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpWcATGFH_ey"
      },
      "source": [
        "### Visualize FN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlowP7nSIo2w"
      },
      "source": [
        "# 앞서데이터 전처리과정에서에서 우리는 28 by 28 이차원 이미지데이터 1차원 벡터로 Reshape 시켜놓았다. \n",
        "# 이미지 플로팅을 위해 우리는 다시 이미지를 이차원으로 변화 시킨다.\n",
        "\n",
        "x_test = X_test.reshape(10000, 28,28) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOnZLSTcuFVd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Plot FN classified images\n",
        "\n",
        "def show_FN(x_test,y_test, y_pred, true_class):\n",
        "\n",
        "  dict = {'True Class': y_test,'Predicted Class':y_pred}\n",
        "  df = pd.DataFrame(dict)\n",
        "  df['row_num'] = df.index\n",
        "  df = df[df['True Class']==true_class]\n",
        "  df = df[df['True Class'] != df['Predicted Class']]\n",
        "  n = df.shape[0] # sample size\n",
        "  print(\"Total Rows = \",n)\n",
        "  print(df)\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 20))\n",
        "\n",
        "  cols = 5\n",
        "  rows = int(n/cols) +1\n",
        "\n",
        "  for i in range(n):\n",
        "    ax = plt.subplot(rows, cols, i+1)\n",
        "    plt.imshow(x_test[df.iat[i,2]], cmap='gray')\n",
        "    plt.title(\"True: {} Pred: {} \".format(df.iat[i,0],df.iat[i,1]))\n",
        "    plt.axis(False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgywCJcZyKJr"
      },
      "source": [
        "show_FN(x_test,y_test, y_pred, 1)  # Check for 2, 8, 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzpfnpWqIWIR"
      },
      "source": [
        "### Visualize FP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BShJyovKk8c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# Plot FP classified images\n",
        "\n",
        "def show_FP(x_test,y_test, y_pred, pred_class):\n",
        "\n",
        "  dict = {'True Class': y_test,'Predicted Class':y_pred}\n",
        "  df = pd.DataFrame(dict)\n",
        "  df['row_num'] = df.index\n",
        "  df = df[df['Predicted Class']==pred_class]\n",
        "  df = df[df['True Class'] != df['Predicted Class']]\n",
        "  n = df.shape[0] # sample size\n",
        "  print(\"Total Rows = \",n)\n",
        "  print(df)\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 20))\n",
        "\n",
        "  cols = 5\n",
        "  rows = int(n/cols) +1\n",
        "\n",
        "  for i in range(n):\n",
        "    ax = plt.subplot(rows, cols, i+1)\n",
        "    plt.imshow(x_test[df.iat[i,2]], cmap='gray')\n",
        "    plt.title(\"Pred {} True: {} \".format(df.iat[i,1],df.iat[i,0]))\n",
        "    plt.axis(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKSw70ImIUym"
      },
      "source": [
        "show_FP(x_test,y_test, y_pred, 3) # Check for cases predicted as 3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruWjp-KJUfaX"
      },
      "source": [
        "show_FP(x_test,y_test, y_pred, 7)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}